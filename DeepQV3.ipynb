{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzZXXH_ww0zP",
        "outputId": "e350f2dc-19aa-443e-865e-296ab38a50dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ta\n",
            "  Downloading ta-0.11.0.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ta) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from ta) (2.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->ta) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->ta) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->ta) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->ta) (1.17.0)\n",
            "Building wheels for collected packages: ta\n",
            "  Building wheel for ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ta: filename=ta-0.11.0-py3-none-any.whl size=29412 sha256=2009e6356c5c0b3b474df57ef920fa1a07b013873a48eb08d7d671ed9f5cf409\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/d7/29/7781cc5eb9a3659d032d7d15bdd0f49d07d2b24fec29f44bc4\n",
            "Successfully built ta\n",
            "Installing collected packages: ta\n",
            "Successfully installed ta-0.11.0\n"
          ]
        }
      ],
      "source": [
        "! pip install ta"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "from collections import deque, namedtuple\n",
        "import matplotlib.pyplot as plt\n",
        "import ta # Technical Analysis library\n",
        "from sklearn.preprocessing import StandardScaler # For state normalization\n",
        "import copy # For deep copying target network\n",
        "from datetime import timedelta\n",
        "\n",
        "# --- Configuration ---\n",
        "TICKER = 'BTC-USD'       # Stock ticker symbol (e.g., 'AAPL', 'BTC-USD')\n",
        "#TICKER = 'BTC-USD' # Uncomment to test crypto\n",
        "\n",
        "# Determine if the ticker is crypto for market hours adjustment\n",
        "IS_CRYPTO = \"-USD\" in TICKER.upper() or \"-USDT\" in TICKER.upper()\n",
        "\n",
        "DATA_PERIOD_TOTAL = '7d' # Total data to download (yfinance 1-min free limit: 7d for stocks, sometimes more for crypto but 7d is safe)\n",
        "TRAIN_DAYS = 5           # Number of days from downloaded data for training (must be < total days in DATA_PERIOD_TOTAL for a test set)\n",
        "INTERVAL = '1m'          # Data interval\n",
        "N_SHORT_LAGS = 5         # Number of short-term (1-min) lagged features\n",
        "\n",
        "# Adjust minutes in a day based on asset type\n",
        "MINUTES_IN_TRADING_DAY_STOCK = 390 # Approx. 6.5 hours * 60 minutes (e.g., 9:30 AM - 4:00 PM)\n",
        "MINUTES_IN_TRADING_DAY_CRYPTO = 24 * 60 # Crypto trades 24/7\n",
        "MINUTES_IN_DAY_EFFECTIVE = MINUTES_IN_TRADING_DAY_CRYPTO if IS_CRYPTO else MINUTES_IN_TRADING_DAY_STOCK\n",
        "\n",
        "\n",
        "INITIAL_BALANCE = 10000\n",
        "TRANSACTION_COST_PCT = 0.001 # 0.1%\n",
        "\n",
        "# --- DQN Hyperparameters ---\n",
        "BUFFER_SIZE = int(1e5)\n",
        "BATCH_SIZE = 64\n",
        "GAMMA = 0.99\n",
        "LR = 5e-4\n",
        "TAU = 1e-3\n",
        "UPDATE_EVERY = 4\n",
        "TARGET_UPDATE_EVERY = 100 # Soft update target network periodically\n",
        "\n",
        "# --- Exploration ---\n",
        "EPS_START = 1.0\n",
        "EPS_END = 0.01\n",
        "EPS_DECAY = 0.995 # Adjust based on number of episodes / steps per episode\n",
        "\n",
        "# --- Training ---\n",
        "NUM_EPISODES = 500      # Number of training episodes\n",
        "# MAX_T will be determined by the length of the training data for each episode\n",
        "\n",
        "# --- Device ---\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"Ticker: {TICKER}, Crypto: {IS_CRYPTO}, Effective Minutes in Day for Lag: {MINUTES_IN_DAY_EFFECTIVE}\")\n",
        "\n",
        "# --- Data Loading and Feature Engineering ---\n",
        "def load_and_prepare_data(ticker, period, interval, n_short_lags, train_days_count, minutes_in_day_for_lag):\n",
        "    \"\"\"Loads 1-min data, calculates features, adds time features and lags, splits, and scales.\"\"\"\n",
        "    print(f\"Loading data for {ticker} ({period}, {interval})...\")\n",
        "    stock = yf.Ticker(ticker)\n",
        "    data = stock.history(period=period, interval=interval, auto_adjust=True)\n",
        "\n",
        "    if data.empty:\n",
        "        data_source_note = \"Yfinance might have limitations for 1-min data for this crypto/period combination (max 7 days usually for free tier).\" if IS_CRYPTO else \"\"\n",
        "        raise ValueError(f\"No data loaded for {ticker}. Check ticker, period '{period}', interval '{interval}'. {data_source_note}\")\n",
        "    if not isinstance(data.index, pd.DatetimeIndex):\n",
        "        raise ValueError(\"Data index is not a DatetimeIndex. Required for time features.\")\n",
        "\n",
        "    print(f\"Original data shape: {data.shape}\")\n",
        "    data.dropna(subset=['Close', 'Open', 'High', 'Low', 'Volume'], how='any', inplace=True) # Drop rows if essential price/volume data is missing\n",
        "    print(f\"Data shape after initial essential column dropna: {data.shape}\")\n",
        "\n",
        "    if data.empty:\n",
        "        raise ValueError(\"No data left after initial essential column NaN drop.\")\n",
        "\n",
        "    # --- Time Features ---\n",
        "    print(\"Adding time features...\")\n",
        "    data['day_of_week'] = data.index.dayofweek / 6.0\n",
        "    data['hour'] = data.index.hour / 23.0\n",
        "    data['minute'] = data.index.minute / 59.0\n",
        "\n",
        "    # --- Technical Indicators ---\n",
        "    print(\"Calculating technical indicators...\")\n",
        "    data['RSI'] = ta.momentum.RSIIndicator(close=data['Close'], window=14).rsi()\n",
        "    macd = ta.trend.MACD(close=data['Close'], window_slow=26, window_fast=12, window_sign=9)\n",
        "    data['MACD'] = macd.macd()\n",
        "    data['MACD_signal'] = macd.macd_signal()\n",
        "    data['MACD_diff'] = macd.macd_diff()\n",
        "    bb = ta.volatility.BollingerBands(close=data['Close'], window=20, window_dev=2)\n",
        "    data['BB_high_ind'] = bb.bollinger_hband_indicator().astype(float)\n",
        "    data['BB_low_ind'] = bb.bollinger_lband_indicator().astype(float)\n",
        "    data['ATR'] = ta.volatility.AverageTrueRange(high=data['High'], low=data['Low'], close=data['Close'], window=14).average_true_range()\n",
        "\n",
        "    base_feature_cols = ['Close', 'Volume', 'RSI', 'MACD', 'MACD_signal', 'MACD_diff',\n",
        "                           'BB_high_ind', 'BB_low_ind', 'ATR',\n",
        "                           'day_of_week', 'hour', 'minute']\n",
        "\n",
        "    for col in base_feature_cols:\n",
        "        if col not in data.columns:\n",
        "            raise ValueError(f\"Column '{col}' not found in data after indicator calculation.\")\n",
        "        if not pd.api.types.is_numeric_dtype(data[col]):\n",
        "            print(f\"Warning: Column '{col}' is not numeric. Attempting conversion.\")\n",
        "            data[col] = pd.to_numeric(data[col], errors='coerce')\n",
        "\n",
        "    data.dropna(inplace=True) # Drop NaNs from TIs\n",
        "    print(f\"Data shape after TIs and NaN drop: {data.shape}\")\n",
        "    if data.empty:\n",
        "        raise ValueError(\"No data left after calculating TIs and dropping NaNs. Check TI windows or data quality.\")\n",
        "\n",
        "    base_features_df = data[base_feature_cols].copy()\n",
        "\n",
        "    print(f\"Creating lagged features (short, 15m, 1h, 1d-approx based on {minutes_in_day_for_lag} min/day)...\")\n",
        "    all_features_list = [base_features_df]\n",
        "\n",
        "    for lag in range(1, n_short_lags + 1):\n",
        "        shifted = base_features_df.shift(lag)\n",
        "        shifted.columns = [f'{col}_lag{lag}m' for col in base_features_df.columns]\n",
        "        all_features_list.append(shifted)\n",
        "\n",
        "    lags_to_add = {\n",
        "        '15m': 15,\n",
        "        '1h': 60,\n",
        "        '1d': minutes_in_day_for_lag\n",
        "    }\n",
        "    for name, lag_val in lags_to_add.items():\n",
        "        if lag_val <= 0: continue # Skip non-positive lags\n",
        "        if lag_val < len(base_features_df):\n",
        "            shifted = base_features_df.shift(lag_val)\n",
        "            shifted.columns = [f'{col}_lag{name}' for col in base_features_df.columns]\n",
        "            all_features_list.append(shifted)\n",
        "        else:\n",
        "             print(f\"Warning: Lag value {lag_val} for '{name}' is too large for current data length {len(base_features_df)}. Skipping this lag.\")\n",
        "\n",
        "    full_features = pd.concat(all_features_list, axis=1)\n",
        "    original_len_before_lag_dropna = len(full_features)\n",
        "    full_features.dropna(inplace=True)\n",
        "    data = data[data.index.isin(full_features.index)]\n",
        "    print(f\"Data shape after feature engineering and final NaN drop: {full_features.shape}\")\n",
        "    print(f\"Dropped {original_len_before_lag_dropna - len(full_features)} rows due to NaNs from TIs/lags.\")\n",
        "\n",
        "    if full_features.empty or data.empty:\n",
        "        raise ValueError(\"No data left after feature engineering and NaN drop. Adjust lag settings or increase data period.\")\n",
        "\n",
        "    print(f\"Splitting data: {train_days_count} days for training, rest for testing...\")\n",
        "    unique_dates = sorted(data.index.normalize().unique())\n",
        "\n",
        "    if len(unique_dates) <= train_days_count:\n",
        "        raise ValueError(f\"Not enough unique days in data ({len(unique_dates)}) for {train_days_count} train days and a separate test set. Need at least {train_days_count + 1} unique days.\")\n",
        "\n",
        "    split_date_marker = unique_dates[train_days_count - 1]\n",
        "    train_mask = data.index.normalize() <= split_date_marker\n",
        "\n",
        "    first_test_date = unique_dates[train_days_count]\n",
        "    test_mask = data.index.normalize() >= first_test_date\n",
        "\n",
        "    train_stock_data = data[train_mask]\n",
        "    train_features_unscaled = full_features[train_mask]\n",
        "    test_stock_data = data[test_mask]\n",
        "    test_features_unscaled = full_features[test_mask]\n",
        "\n",
        "    if train_stock_data.empty or train_features_unscaled.empty:\n",
        "        raise ValueError(\"Training data is empty after split. Check date splitting logic or data range.\")\n",
        "    if test_stock_data.empty or test_features_unscaled.empty:\n",
        "        print(\"Warning: Test data is empty after split. This might happen if TRAIN_DAYS covers almost all available data or due to data gaps on test days.\")\n",
        "\n",
        "    print(f\"Training data shape: {train_stock_data.shape}, Training features shape: {train_features_unscaled.shape}\")\n",
        "    print(f\"Test data shape: {test_stock_data.shape}, Test features shape: {test_features_unscaled.shape}\")\n",
        "\n",
        "    print(\"Scaling features (fitting on training data only)...\")\n",
        "    scaler = StandardScaler()\n",
        "    scaled_train_features_np = scaler.fit_transform(train_features_unscaled)\n",
        "    scaled_train_features = pd.DataFrame(scaled_train_features_np, index=train_features_unscaled.index, columns=train_features_unscaled.columns)\n",
        "\n",
        "    scaled_test_features = pd.DataFrame()\n",
        "    if not test_features_unscaled.empty:\n",
        "        scaled_test_features_np = scaler.transform(test_features_unscaled)\n",
        "        scaled_test_features = pd.DataFrame(scaled_test_features_np, index=test_features_unscaled.index, columns=test_features_unscaled.columns)\n",
        "    else:\n",
        "        print(\"No test data to scale.\")\n",
        "\n",
        "    return (train_stock_data, scaled_train_features,\n",
        "            test_stock_data, scaled_test_features, scaler)\n",
        "\n",
        "# --- Trading Environment ---\n",
        "class TradingEnv:\n",
        "    def __init__(self, stock_data_df, feature_data_df, initial_balance=10000, transaction_cost_pct=0.001):\n",
        "        self.stock_data = stock_data_df.copy().reset_index(drop=True)\n",
        "        self.feature_data = feature_data_df.copy().reset_index(drop=True)\n",
        "\n",
        "        if not self.stock_data.index.equals(self.feature_data.index):\n",
        "             # This can happen if one df is empty and the other is not before reset_index\n",
        "            if self.stock_data.empty and self.feature_data.empty:\n",
        "                print(\"Warning: Both stock_data and feature_data are empty in TradingEnv init.\")\n",
        "            elif self.stock_data.empty:\n",
        "                 raise ValueError(\"Stock data is empty while feature data is not.\")\n",
        "            elif self.feature_data.empty:\n",
        "                 raise ValueError(\"Feature data is empty while stock data is not.\")\n",
        "            else:\n",
        "                # If both are non-empty but indices don't match after reset, it's an issue\n",
        "                raise ValueError(f\"Stock data and feature data indices do not match after reset. Stock len: {len(self.stock_data)}, Feature len: {len(self.feature_data)}\")\n",
        "\n",
        "\n",
        "        self.initial_balance = initial_balance\n",
        "        self.transaction_cost_pct = transaction_cost_pct\n",
        "\n",
        "        if self.feature_data.empty:\n",
        "            print(\"Warning: Feature data is empty, environment will have 0 steps.\")\n",
        "            self.n_steps = 0\n",
        "            self.state_dim = 0 # Or a predefined dimension if known, but safer to error out or handle\n",
        "        else:\n",
        "            self.n_steps = len(self.feature_data)\n",
        "            self.state_dim = self.feature_data.shape[1]\n",
        "\n",
        "        self.action_space_n = 3 # 0: Hold, 1: Buy, 2: Sell\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self):\n",
        "        self.current_step = 0\n",
        "        self.balance = self.initial_balance\n",
        "        self.shares_held = 0\n",
        "        self.net_worth = self.initial_balance\n",
        "        self.trade_history = []\n",
        "        self.total_reward = 0\n",
        "        # self.daily_net_worths = [self.initial_balance] # Removed, use history in backtest for plotting\n",
        "        return self._get_state()\n",
        "\n",
        "    def _get_state(self):\n",
        "        if self.n_steps == 0: # Handle case where environment has no data\n",
        "             return np.zeros(self.state_dim if hasattr(self, 'state_dim') and self.state_dim > 0 else 1) # Return dummy state\n",
        "        if self.current_step < self.n_steps:\n",
        "            return self.feature_data.iloc[self.current_step].values\n",
        "        else:\n",
        "            return np.zeros(self.state_dim)\n",
        "\n",
        "    def _get_current_price(self):\n",
        "        if self.n_steps == 0 or self.current_step >= len(self.stock_data):\n",
        "            return 0 # Or raise error, handle gracefully\n",
        "        return self.stock_data['Close'].iloc[self.current_step]\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.n_steps == 0 or self.current_step >= self.n_steps -1 : # Cannot act on the last step as no next state price for reward\n",
        "             # If state_dim is 0 due to no features, get_state will handle it\n",
        "             return self._get_state(), 0, True, {}\n",
        "\n",
        "        prev_net_worth = self.net_worth\n",
        "        current_price = self._get_current_price()\n",
        "        cost = 0\n",
        "        trade_executed = False\n",
        "        action_type = 'HOLD'\n",
        "\n",
        "        if action == 1: # Buy\n",
        "            shares_can_buy = 0\n",
        "            if current_price > 0: # Avoid division by zero\n",
        "                shares_can_buy = self.balance / (current_price * (1 + self.transaction_cost_pct))\n",
        "\n",
        "            # Ensure we buy a positive amount of shares and have balance\n",
        "            if shares_can_buy > 1e-8 and self.balance > 0: # 1e-8 is a small threshold to avoid dust trades\n",
        "                shares_to_buy = shares_can_buy # Buy all possible (fractional)\n",
        "\n",
        "                actual_cost_of_shares = shares_to_buy * current_price\n",
        "                transaction_fee = actual_cost_of_shares * self.transaction_cost_pct\n",
        "\n",
        "                if self.balance >= actual_cost_of_shares + transaction_fee: # Final check\n",
        "                    self.shares_held += shares_to_buy\n",
        "                    self.balance -= (actual_cost_of_shares + transaction_fee)\n",
        "                    cost = transaction_fee\n",
        "                    self.trade_history.append({'step': self.current_step, 'type': 'BUY',\n",
        "                                               'price': current_price, 'shares': shares_to_buy, 'cost': cost})\n",
        "                    trade_executed = True\n",
        "                    action_type = 'BUY'\n",
        "                else: # Should not happen if shares_can_buy logic is correct, but as a safeguard\n",
        "                    action_type = 'HOLD (Buy failed - insufficient funds for tx cost)'\n",
        "            else:\n",
        "                action_type = 'HOLD (Buy cond. not met)'\n",
        "\n",
        "\n",
        "        elif action == 2: # Sell\n",
        "            if self.shares_held > 1e-8: # Sell if holding a meaningful amount\n",
        "                sell_value = self.shares_held * current_price\n",
        "                transaction_fee = sell_value * self.transaction_cost_pct\n",
        "\n",
        "                self.balance += (sell_value - transaction_fee)\n",
        "                sold_shares = self.shares_held\n",
        "                self.shares_held = 0 # Sell all\n",
        "                cost = transaction_fee\n",
        "                self.trade_history.append({'step': self.current_step, 'type': 'SELL',\n",
        "                                           'price': current_price, 'shares': sold_shares, 'cost': cost})\n",
        "                trade_executed = True\n",
        "                action_type = 'SELL'\n",
        "            else:\n",
        "                action_type = 'HOLD (Sell cond. not met)'\n",
        "\n",
        "\n",
        "        self.current_step += 1\n",
        "\n",
        "        next_price_for_eval = current_price # Default if at the end\n",
        "        if self.current_step < self.n_steps: # If there's a next step\n",
        "            next_price_for_eval = self.stock_data['Close'].iloc[self.current_step]\n",
        "\n",
        "        self.net_worth = self.balance + self.shares_held * next_price_for_eval\n",
        "        reward = self.net_worth - prev_net_worth\n",
        "        done = self.current_step >= self.n_steps -1 or self.net_worth <= 0\n",
        "\n",
        "        next_state = self._get_state()\n",
        "        self.total_reward += reward\n",
        "        # self.daily_net_worths.append(self.net_worth) # Removed\n",
        "\n",
        "        info = {\n",
        "            'step': self.current_step, # This is already the *next* step index\n",
        "            'balance': self.balance,\n",
        "            'shares_held': self.shares_held,\n",
        "            'net_worth': self.net_worth,\n",
        "            'trade_executed': trade_executed,\n",
        "            'action_taken_code': action, # Original action code\n",
        "            'action_type_str': action_type, # String representation of what happened\n",
        "            'cost': cost,\n",
        "            'current_price_of_trade': current_price if trade_executed else None\n",
        "        }\n",
        "        return next_state, reward, done, info\n",
        "\n",
        "    def render(self, mode='human', **kwargs):\n",
        "        # Step in info is already advanced, so use current_step-1 for \"current action\" context\n",
        "        print(f\"Step: {self.current_step-1}/{self.n_steps}, Net Worth: {self.net_worth:.2f}, Shares: {self.shares_held:.4f}, Balance: {self.balance:.2f}, Total Episode Reward: {self.total_reward:.2f}\")\n",
        "\n",
        "# --- Q-Network (Unchanged) ---\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size, seed, fc1_units=128, fc2_units=128, fc3_units=64):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
        "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
        "        self.fc3 = nn.Linear(fc2_units, fc3_units)\n",
        "        self.fc4 = nn.Linear(fc3_units, action_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        return self.fc4(x)\n",
        "\n",
        "# --- Replay Buffer (Unchanged) ---\n",
        "Experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        e = Experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def sample(self):\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "# --- DQN Agent (Modified to return loss and Q-values) ---\n",
        "class DQNAgent():\n",
        "    def __init__(self, state_size, action_size, seed):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "        if state_size == 0: # Handle case of no state from env\n",
        "            print(\"Warning: DQNAgent initialized with state_size 0. Network will not be functional.\")\n",
        "            # Create dummy networks to avoid crashing, but they won't work\n",
        "            self.qnetwork_local = nn.Linear(1,action_size).to(device) # Dummy\n",
        "            self.qnetwork_target = nn.Linear(1,action_size).to(device) # Dummy\n",
        "            self.optimizer = None\n",
        "        else:\n",
        "            self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
        "            self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
        "            self.qnetwork_target.load_state_dict(self.qnetwork_local.state_dict())\n",
        "            self.qnetwork_target.eval()\n",
        "            self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
        "\n",
        "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
        "        self.t_step = 0\n",
        "        self.target_update_step = 0\n",
        "\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        if self.state_size == 0: return None # Agent not functional\n",
        "\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
        "        current_loss = None\n",
        "        if self.t_step == 0:\n",
        "            if len(self.memory) > BATCH_SIZE:\n",
        "                experiences = self.memory.sample()\n",
        "                current_loss = self.learn(experiences, GAMMA)\n",
        "\n",
        "        self.target_update_step = (self.target_update_step + 1) % TARGET_UPDATE_EVERY\n",
        "        if self.target_update_step == 0:\n",
        "             self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)\n",
        "        return current_loss\n",
        "\n",
        "    def act(self, state, eps=0.):\n",
        "        if self.state_size == 0: return random.choice(np.arange(self.action_size)), 0 # Random action if not functional\n",
        "\n",
        "        state_t = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        self.qnetwork_local.eval()\n",
        "        with torch.no_grad():\n",
        "            action_values = self.qnetwork_local(state_t)\n",
        "        self.qnetwork_local.train()\n",
        "\n",
        "        if random.random() > eps:\n",
        "            action = np.argmax(action_values.cpu().data.numpy())\n",
        "        else:\n",
        "            action = random.choice(np.arange(self.action_size))\n",
        "        return action, action_values.cpu().data.numpy().squeeze().mean() # Squeeze for single action value array\n",
        "\n",
        "    def learn(self, experiences, gamma):\n",
        "        if self.optimizer is None: return 0.0 # Cannot learn\n",
        "\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
        "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
        "\n",
        "        loss = F.mse_loss(Q_expected, Q_targets)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(self.qnetwork_local.parameters(), 1) # Optional: Gradient Clipping\n",
        "        self.optimizer.step()\n",
        "        return loss.item()\n",
        "\n",
        "    def soft_update(self, local_model, target_model, tau):\n",
        "        if self.state_size == 0: return # Agent not functional\n",
        "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
        "\n",
        "# --- Training Function (Modified for more metrics) ---\n",
        "def train_dqn(agent, env, n_episodes=2000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
        "    if env.n_steps == 0:\n",
        "        print(\"Environment has no steps. Skipping training.\")\n",
        "        return { 'rewards': [], 'net_worths': [], 'losses': [], 'avg_q_values': [], 'epsilons': [], 'num_trades': []}\n",
        "\n",
        "    episode_rewards = []\n",
        "    avg_rewards_deque = deque(maxlen=100) # For printing average over last 100 episodes\n",
        "    episode_net_worths = []\n",
        "    episode_losses = []\n",
        "    episode_avg_q_values = []\n",
        "    episode_epsilons = []\n",
        "    episode_num_trades = []\n",
        "\n",
        "    eps = eps_start\n",
        "    max_t_per_episode = env.n_steps - 1 # Max steps per episode is length of data for that episode minus 1\n",
        "\n",
        "    print(f\"\\nStarting Training for {n_episodes} episodes... Max steps per episode: {max_t_per_episode}\")\n",
        "    if max_t_per_episode <=0:\n",
        "        print(\"Max steps per episode is <=0. Training cannot proceed.\")\n",
        "        return { 'rewards': [], 'net_worths': [], 'losses': [], 'avg_q_values': [], 'epsilons': [], 'num_trades': []}\n",
        "\n",
        "    for i_episode in range(1, n_episodes + 1):\n",
        "        state = env._reset()\n",
        "        current_episode_reward = 0\n",
        "        episode_loss_sum = 0\n",
        "        episode_q_sum = 0\n",
        "        learn_steps_count = 0\n",
        "        q_value_steps_count = 0\n",
        "        num_trades_this_episode = 0\n",
        "\n",
        "        for t in range(max_t_per_episode): # Iterate up to n_steps-2 (0 to n_steps-2)\n",
        "            action, q_value = agent.act(state, eps)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            loss = agent.step(state, action, reward, next_state, done)\n",
        "\n",
        "            state = next_state\n",
        "            current_episode_reward += reward\n",
        "\n",
        "            if info.get('trade_executed', False):\n",
        "                num_trades_this_episode +=1\n",
        "\n",
        "            if loss is not None:\n",
        "                episode_loss_sum += loss\n",
        "                learn_steps_count += 1\n",
        "            if q_value is not None: # q_value is now scalar (mean)\n",
        "                episode_q_sum += q_value\n",
        "                q_value_steps_count +=1\n",
        "\n",
        "            # Optional: Detailed print for debugging first few steps of first crypto episode\n",
        "            # if IS_CRYPTO and i_episode == 1 and t < 5:\n",
        "            #    print(f\"  Crypto Debug Ep1, t={t}: action={action}, Q_mean={q_value:.2f}, reward={reward:.2f}, bal={env.balance:.2f}, shares={env.shares_held:.4f}, net_w={env.net_worth:.2f}, action_str='{info['action_type_str']}'\")\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        avg_rewards_deque.append(current_episode_reward)\n",
        "        episode_rewards.append(current_episode_reward)\n",
        "        episode_net_worths.append(env.net_worth)\n",
        "        episode_epsilons.append(eps)\n",
        "        episode_num_trades.append(num_trades_this_episode)\n",
        "\n",
        "\n",
        "        if learn_steps_count > 0:\n",
        "            episode_losses.append(episode_loss_sum / learn_steps_count)\n",
        "        else:\n",
        "            episode_losses.append(None)\n",
        "\n",
        "        if q_value_steps_count > 0:\n",
        "            episode_avg_q_values.append(episode_q_sum / q_value_steps_count)\n",
        "        else:\n",
        "            episode_avg_q_values.append(None)\n",
        "\n",
        "        eps = max(eps_end, eps_decay * eps)\n",
        "\n",
        "        print(f'\\rEpisode {i_episode}/{n_episodes}\\tAvg Reward (100): {np.mean(avg_rewards_deque):.2f}\\tReward: {current_episode_reward:.2f}\\tNet Worth: {env.net_worth:.2f}\\tEps: {eps:.4f}\\tTrades: {num_trades_this_episode}', end=\"\")\n",
        "        if i_episode % 20 == 0 or i_episode == n_episodes :\n",
        "            print(f'\\rEpisode {i_episode}/{n_episodes}\\tAvg Reward (100): {np.mean(avg_rewards_deque):.2f}\\tReward: {current_episode_reward:.2f}\\tNet Worth: {env.net_worth:.2f}\\tEps: {eps:.4f}\\tTrades: {num_trades_this_episode}')\n",
        "\n",
        "    print(\"\\nTraining finished.\")\n",
        "    return {\n",
        "        'rewards': episode_rewards, 'net_worths': episode_net_worths,\n",
        "        'losses': episode_losses, 'avg_q_values': episode_avg_q_values,\n",
        "        'epsilons': episode_epsilons, 'num_trades': episode_num_trades\n",
        "    }\n",
        "\n",
        "# --- Backtesting Function (Modified for more plots) ---\n",
        "def backtest(agent, env, stock_data_original_index):\n",
        "    if env.n_steps == 0:\n",
        "        print(\"Environment has no steps. Skipping backtesting.\")\n",
        "        return env.initial_balance, 0, []\n",
        "\n",
        "    print(\"\\nStarting Backtesting...\")\n",
        "    state = env._reset()\n",
        "    done = False\n",
        "\n",
        "    # Store history for plotting and analysis\n",
        "    portfolio_values = [env.initial_balance] # Start with initial balance\n",
        "    all_shares_held = [0]\n",
        "    all_rewards = []\n",
        "    all_actions_str = [] # Store string representation of actions\n",
        "\n",
        "    max_backtest_steps = env.n_steps -1\n",
        "    if max_backtest_steps <= 0:\n",
        "        print(\"Not enough data for backtesting (max_backtest_steps <=0).\")\n",
        "        return env.initial_balance, 0, []\n",
        "\n",
        "    for t_step in range(max_backtest_steps): # Iterate up to n_steps-2\n",
        "        action, _ = agent.act(state, eps=0.0) # No exploration in backtest\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        state = next_state\n",
        "\n",
        "        portfolio_values.append(info['net_worth'])\n",
        "        all_shares_held.append(info['shares_held'])\n",
        "        all_rewards.append(reward)\n",
        "        all_actions_str.append(info['action_type_str'])\n",
        "\n",
        "        if info.get('trade_executed', False):\n",
        "             print(f\"Step: {info['step']-1}, Action: {info['action_type_str']}, Price: {info['current_price_of_trade']:.2f}, Shares: {info['shares_held']:.4f} (after trade), Net Worth: {info['net_worth']:.2f}, Cost: {info['cost']:.2f}\")\n",
        "        elif (info['step']-1) % (MINUTES_IN_DAY_EFFECTIVE // 4) == 0: # Print Hold status occasionally based on effective day length\n",
        "             print(f\"Step: {info['step']-1}, Action: {info['action_type_str']}, Net Worth: {info['net_worth']:.2f}\")\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    trade_details = env.trade_history # Get trade log from environment\n",
        "    print(\"Backtesting Finished.\")\n",
        "\n",
        "    final_net_worth = portfolio_values[-1]\n",
        "    total_return_pct = (final_net_worth - env.initial_balance) / env.initial_balance * 100 if env.initial_balance else 0\n",
        "\n",
        "    buy_hold_final_worth = env.initial_balance\n",
        "    buy_hold_return_pct = 0.0\n",
        "    if not env.stock_data.empty and len(env.stock_data) > 1:\n",
        "        buy_hold_start_price = env.stock_data['Close'].iloc[0]\n",
        "        buy_hold_end_price = env.stock_data['Close'].iloc[env.n_steps-1 if env.n_steps > 0 else 0] # Use last available price\n",
        "        if buy_hold_start_price > 0:\n",
        "            buy_hold_shares = env.initial_balance / buy_hold_start_price\n",
        "            buy_hold_final_worth = buy_hold_shares * buy_hold_end_price\n",
        "            buy_hold_return_pct = (buy_hold_final_worth - env.initial_balance) / env.initial_balance * 100 if env.initial_balance else 0\n",
        "\n",
        "    print(f\"\\n--- Backtest Results ({TICKER}) ---\")\n",
        "    print(f\"Initial Balance: ${env.initial_balance:.2f}\")\n",
        "    print(f\"Final Net Worth (Agent): ${final_net_worth:.2f}\")\n",
        "    print(f\"Total Return (Agent): {total_return_pct:.2f}%\")\n",
        "    print(f\"Number of Trades (Agent): {len(trade_details)}\")\n",
        "    print(f\"\\n--- Buy and Hold Benchmark ---\")\n",
        "    print(f\"Final Net Worth (Buy & Hold): ${buy_hold_final_worth:.2f}\")\n",
        "    print(f\"Total Return (Buy & Hold): {buy_hold_return_pct:.2f}%\")\n",
        "\n",
        "    # Plotting\n",
        "    # The length of portfolio_values is num_steps + 1 (includes initial balance)\n",
        "    # The length of stock_data_original_index is num_env_data_points\n",
        "    # env.n_steps is len(feature_data) which is also len(stock_data) in env\n",
        "    # We have max_backtest_steps = env.n_steps - 1. Loop runs this many times.\n",
        "    # portfolio_values will have 1 (initial) + max_backtest_steps = env.n_steps items.\n",
        "    # So, use stock_data_original_index[:env.n_steps]\n",
        "\n",
        "    plot_index = stock_data_original_index[:len(portfolio_values)-1] # Index for items that correspond to a step/action\n",
        "    # This plot_index should match the length of rewards, shares_held (excluding initial), etc.\n",
        "\n",
        "    plt.figure(figsize=(18, 16))\n",
        "\n",
        "    plt.subplot(4, 1, 1)\n",
        "    # portfolio_values[0] is initial balance. portfolio_values[1:] are after each step.\n",
        "    plt.plot(plot_index, portfolio_values[1:], label='Agent Portfolio Value', color='blue')\n",
        "    if not env.stock_data.empty and len(env.stock_data) > 1 and buy_hold_start_price > 0:\n",
        "        # Align Buy & Hold with the actual period agent traded on\n",
        "        buy_hold_plot_prices = env.stock_data['Close'].iloc[:len(plot_index)].values\n",
        "        buy_hold_values = (env.initial_balance / buy_hold_start_price) * buy_hold_plot_prices\n",
        "        plt.plot(plot_index, buy_hold_values, label=f'Buy & Hold ({TICKER})', color='grey', linestyle='--')\n",
        "    plt.title(f'{TICKER} Agent Performance vs Buy & Hold (Backtest)')\n",
        "    plt.ylabel('Portfolio Value ($)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(4, 1, 2)\n",
        "    prices_to_plot = []\n",
        "    if not env.stock_data.empty:\n",
        "        prices_to_plot = env.stock_data['Close'].iloc[:len(plot_index)].values\n",
        "        plt.plot(plot_index, prices_to_plot, label='Stock Price', color='black', alpha=0.7)\n",
        "\n",
        "        buy_trade_details = [td for td in trade_details if td['type'] == 'BUY']\n",
        "        sell_trade_details = [td for td in trade_details if td['type'] == 'SELL']\n",
        "\n",
        "        if buy_trade_details:\n",
        "            buy_steps_indices = [td['step'] for td in buy_trade_details if td['step'] < len(plot_index)]\n",
        "            buy_prices_at_steps = [td['price'] for td in buy_trade_details if td['step'] < len(plot_index)]\n",
        "            if buy_steps_indices: # Ensure valid indices exist\n",
        "                 plt.scatter(plot_index[buy_steps_indices], buy_prices_at_steps, marker='^', color='green', label='Buy Signal', s=100, alpha=0.9, zorder=5)\n",
        "\n",
        "        if sell_trade_details:\n",
        "            sell_steps_indices = [td['step'] for td in sell_trade_details if td['step'] < len(plot_index)]\n",
        "            sell_prices_at_steps = [td['price'] for td in sell_trade_details if td['step'] < len(plot_index)]\n",
        "            if sell_steps_indices: # Ensure valid indices exist\n",
        "                plt.scatter(plot_index[sell_steps_indices], sell_prices_at_steps, marker='v', color='red', label='Sell Signal', s=100, alpha=0.9, zorder=5)\n",
        "\n",
        "    plt.ylabel('Stock Price ($)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(4, 1, 3)\n",
        "    # all_shares_held[0] is initial. all_shares_held[1:] are after each step.\n",
        "    plt.plot(plot_index, all_shares_held[1:], label='Shares Held', color='purple')\n",
        "    plt.ylabel('Number of Shares')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    ax1 = plt.subplot(4, 1, 4)\n",
        "    # all_rewards has length max_backtest_steps.\n",
        "    if all_rewards:\n",
        "        cumulative_rewards = np.cumsum(all_rewards)\n",
        "        color = 'tab:green'\n",
        "        ax1.set_xlabel('Time (Datetime)')\n",
        "        ax1.set_ylabel('Cumulative Reward', color=color)\n",
        "        ax1.plot(plot_index[:len(cumulative_rewards)], cumulative_rewards, color=color, label='Cumulative Reward')\n",
        "        ax1.tick_params(axis='y', labelcolor=color)\n",
        "        ax1.legend(loc='upper left')\n",
        "        ax1.grid(True, axis='y')\n",
        "\n",
        "        ax2 = ax1.twinx()\n",
        "        portfolio_values_for_drawdown = np.array(portfolio_values[1:]) # Net worth after each step\n",
        "        if len(portfolio_values_for_drawdown) > 0:\n",
        "            peak = np.maximum.accumulate(portfolio_values_for_drawdown)\n",
        "            drawdown = (portfolio_values_for_drawdown - peak) / peak * 100 # Percentage\n",
        "            color = 'tab:red'\n",
        "            ax2.set_ylabel('Drawdown (%)', color=color)\n",
        "            ax2.plot(plot_index[:len(drawdown)], drawdown, color=color, alpha=0.6, label='Drawdown')\n",
        "            ax2.tick_params(axis='y', labelcolor=color)\n",
        "            ax2.fill_between(plot_index[:len(drawdown)], drawdown, 0, alpha=0.2, color=color)\n",
        "            ax2.legend(loc='upper right')\n",
        "\n",
        "    plt.title('Agent Financial Metrics')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return final_net_worth, total_return_pct, trade_details\n",
        "\n",
        "\n",
        "# --- Plot Training Metrics ---\n",
        "def plot_training_metrics(metrics, ticker_symbol):\n",
        "    # Filter out metrics that are empty or None before counting\n",
        "    valid_metric_keys = [k for k, v in metrics.items() if v and (isinstance(v, list) and len(v) > 0)]\n",
        "    num_metrics_to_plot = len(valid_metric_keys)\n",
        "\n",
        "    if num_metrics_to_plot == 0:\n",
        "        print(\"No valid training metrics to plot.\")\n",
        "        return\n",
        "\n",
        "    fig, axs = plt.subplots(num_metrics_to_plot, 1, figsize=(12, num_metrics_to_plot * 3.5), sharex=True)\n",
        "    if num_metrics_to_plot == 1: axs = [axs] # Ensure axs is always a list/array\n",
        "\n",
        "    plot_idx = 0\n",
        "\n",
        "    def plot_metric(data, label, color, ylabel, rolling_window=100, rolling_label_suffix=\"Avg\"):\n",
        "        nonlocal plot_idx\n",
        "        if data and any(d is not None for d in data): # Check for non-None items\n",
        "            valid_data_points = [(i,val) for i,val in enumerate(data) if val is not None]\n",
        "            if not valid_data_points: return # Skip if no valid points\n",
        "\n",
        "            indices, values = zip(*valid_data_points)\n",
        "            axs[plot_idx].plot(indices, values, label=label, color=color, alpha=0.7)\n",
        "\n",
        "            # Calculate rolling mean only on valid (non-None) numeric data\n",
        "            series = pd.Series(values, index=indices)\n",
        "            if len(series) >= rolling_window:\n",
        "                 axs[plot_idx].plot(series.rolling(window=rolling_window).mean(), label=f'Rolling {rolling_label_suffix} ({rolling_window} eps)', color=plt.cm.Oranges(0.6))\n",
        "\n",
        "            axs[plot_idx].set_ylabel(ylabel)\n",
        "            axs[plot_idx].legend()\n",
        "            axs[plot_idx].grid(True)\n",
        "            if plot_idx == 0: axs[plot_idx].set_title(f'Training Progress for {ticker_symbol}')\n",
        "            plot_idx +=1\n",
        "\n",
        "    plot_metric(metrics.get('rewards'), 'Episode Reward', 'blue', 'Total Reward', rolling_label_suffix=\"Reward\")\n",
        "    plot_metric(metrics.get('net_worths'), 'Episode End Net Worth', 'green', 'Net Worth ($)', rolling_label_suffix=\"Net Worth\")\n",
        "    plot_metric(metrics.get('losses'), 'Avg Episode Loss', 'red', 'MSE Loss', rolling_label_suffix=\"Loss\")\n",
        "    plot_metric(metrics.get('avg_q_values'), 'Avg Q-Value', 'purple', 'Avg Q-Value', rolling_label_suffix=\"Q-Value\")\n",
        "    plot_metric(metrics.get('epsilons'), 'Epsilon', 'cyan', 'Epsilon Value', rolling_window=1, rolling_label_suffix=\"Epsilon\") # No rolling for epsilon typically\n",
        "    plot_metric(metrics.get('num_trades'), 'Number of Trades', 'brown', 'Trades per Episode', rolling_label_suffix=\"Trades\")\n",
        "\n",
        "    if plot_idx > 0 : # If any plot was made\n",
        "        axs[plot_idx-1].set_xlabel('Episode #')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == '__main__':\n",
        "    try:\n",
        "        # 1. Load and Prepare Data\n",
        "        train_stock_df, train_features_scaled_df, \\\n",
        "        test_stock_df, test_features_scaled_df, \\\n",
        "        feature_scaler = load_and_prepare_data(\n",
        "            TICKER, DATA_PERIOD_TOTAL, INTERVAL, N_SHORT_LAGS, TRAIN_DAYS, MINUTES_IN_DAY_EFFECTIVE\n",
        "        )\n",
        "\n",
        "        if train_features_scaled_df.empty:\n",
        "            raise ValueError(\"Training features are empty after loading and preparation. Cannot proceed.\")\n",
        "\n",
        "        # 2. Create Training Environment\n",
        "        train_env = TradingEnv(train_stock_df, train_features_scaled_df,\n",
        "                                INITIAL_BALANCE, TRANSACTION_COST_PCT)\n",
        "\n",
        "        # 3. Create Agent\n",
        "        agent = DQNAgent(state_size=train_env.state_dim, action_size=train_env.action_space_n, seed=0)\n",
        "\n",
        "        if train_env.state_dim == 0: # If env could not be properly initialized\n",
        "            raise ValueError(\"Environment state dimension is 0. Agent cannot be trained. Check data loading and feature engineering.\")\n",
        "\n",
        "        # 4. Train the Agent\n",
        "        training_metrics = train_dqn(\n",
        "            agent, train_env,\n",
        "            n_episodes=NUM_EPISODES,\n",
        "            eps_start=EPS_START,\n",
        "            eps_end=EPS_END,\n",
        "            eps_decay=EPS_DECAY\n",
        "        )\n",
        "\n",
        "        # Plot training progress\n",
        "        plot_training_metrics(training_metrics, TICKER)\n",
        "\n",
        "        # Optional: Save the trained model\n",
        "        model_save_path = f'{TICKER.replace(\"-\",\"_\")}_dqn_model_intraday.pth' # Sanitize filename\n",
        "        torch.save(agent.qnetwork_local.state_dict(), model_save_path)\n",
        "        print(f\"Trained model saved to {model_save_path}\")\n",
        "\n",
        "        # 5. Backtest the Trained Agent\n",
        "        if not test_features_scaled_df.empty and not test_stock_df.empty:\n",
        "            backtest_env = TradingEnv(test_stock_df, test_features_scaled_df,\n",
        "                                      INITIAL_BALANCE, TRANSACTION_COST_PCT)\n",
        "            if backtest_env.state_dim > 0:\n",
        "                final_worth, total_return, trade_history = backtest(agent, backtest_env, test_stock_df.index)\n",
        "            else:\n",
        "                print(\"\\nSkipping backtesting as test environment state dimension is 0.\")\n",
        "        else:\n",
        "            print(\"\\nSkipping backtesting as test data is empty.\")\n",
        "            print(\"This can happen if TRAIN_DAYS covers all available data or if specified test period had no valid data after processing.\")\n",
        "\n",
        "    except ValueError as ve:\n",
        "        print(f\"ValueError: {ve}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rX_TTNs5w52H",
        "outputId": "b142d820-4df0-4e71-9773-a56b67302efe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Ticker: BTC-USD, Crypto: True, Effective Minutes in Day for Lag: 1440\n",
            "Loading data for BTC-USD (7d, 1m)...\n",
            "Original data shape: (7630, 7)\n",
            "Data shape after initial essential column dropna: (7630, 7)\n",
            "Adding time features...\n",
            "Calculating technical indicators...\n",
            "Data shape after TIs and NaN drop: (7597, 17)\n",
            "Creating lagged features (short, 15m, 1h, 1d-approx based on 1440 min/day)...\n",
            "Data shape after feature engineering and final NaN drop: (6157, 108)\n",
            "Dropped 1440 rows due to NaNs from TIs/lags.\n",
            "Splitting data: 5 days for training, rest for testing...\n",
            "Training data shape: (5955, 17), Training features shape: (5955, 108)\n",
            "Test data shape: (202, 17), Test features shape: (202, 108)\n",
            "Scaling features (fitting on training data only)...\n",
            "\n",
            "Starting Training for 500 episodes... Max steps per episode: 5954\n",
            "Episode 20/500\tAvg Reward (100): -4196.28\tReward: -3790.63\tNet Worth: 6209.37\tEps: 0.9046\tTrades: 466\n",
            "Episode 40/500\tAvg Reward (100): -4121.36\tReward: -3945.09\tNet Worth: 6054.91\tEps: 0.8183\tTrades: 516\n",
            "Episode 60/500\tAvg Reward (100): -4009.76\tReward: -3727.93\tNet Worth: 6272.07\tEps: 0.7403\tTrades: 460\n",
            "Episode 80/500\tAvg Reward (100): -3907.27\tReward: -3794.87\tNet Worth: 6205.13\tEps: 0.6696\tTrades: 512\n",
            "Episode 100/500\tAvg Reward (100): -3819.39\tReward: -3393.24\tNet Worth: 6606.76\tEps: 0.6058\tTrades: 482\n",
            "Episode 120/500\tAvg Reward (100): -3612.37\tReward: -3406.66\tNet Worth: 6593.34\tEps: 0.5480\tTrades: 453\n",
            "Episode 140/500\tAvg Reward (100): -3398.62\tReward: -2886.24\tNet Worth: 7113.76\tEps: 0.4957\tTrades: 365\n",
            "Episode 160/500\tAvg Reward (100): -3151.05\tReward: -2598.10\tNet Worth: 7401.90\tEps: 0.4484\tTrades: 374\n",
            "Episode 180/500\tAvg Reward (100): -2918.77\tReward: -2226.30\tNet Worth: 7773.70\tEps: 0.4057\tTrades: 300\n",
            "Episode 200/500\tAvg Reward (100): -2674.33\tReward: -2342.73\tNet Worth: 7657.27\tEps: 0.3670\tTrades: 301\n",
            "Episode 220/500\tAvg Reward (100): -2483.27\tReward: -1881.67\tNet Worth: 8118.33\tEps: 0.3320\tTrades: 270\n",
            "Episode 240/500\tAvg Reward (100): -2274.24\tReward: -1856.59\tNet Worth: 8143.41\tEps: 0.3003\tTrades: 272\n",
            "Episode 260/500\tAvg Reward (100): -2119.17\tReward: -1891.03\tNet Worth: 8108.97\tEps: 0.2716\tTrades: 276\n",
            "Episode 280/500\tAvg Reward (100): -1957.51\tReward: -1408.97\tNet Worth: 8591.03\tEps: 0.2457\tTrades: 218\n",
            "Episode 300/500\tAvg Reward (100): -1818.16\tReward: -1635.69\tNet Worth: 8364.31\tEps: 0.2223\tTrades: 212\n",
            "Episode 320/500\tAvg Reward (100): -1623.50\tReward: -1024.89\tNet Worth: 8975.11\tEps: 0.2011\tTrades: 152\n",
            "Episode 340/500\tAvg Reward (100): -1474.70\tReward: -1149.06\tNet Worth: 8850.94\tEps: 0.1819\tTrades: 164\n",
            "Episode 360/500\tAvg Reward (100): -1331.04\tReward: -976.40\tNet Worth: 9023.60\tEps: 0.1646\tTrades: 176\n",
            "Episode 380/500\tAvg Reward (100): -1199.18\tReward: -862.08\tNet Worth: 9137.92\tEps: 0.1489\tTrades: 174\n",
            "Episode 400/500\tAvg Reward (100): -1037.08\tReward: -925.21\tNet Worth: 9074.79\tEps: 0.1347\tTrades: 164\n",
            "Episode 420/500\tAvg Reward (100): -932.17\tReward: -603.50\tNet Worth: 9396.50\tEps: 0.1218\tTrades: 106\n",
            "Episode 428/500\tAvg Reward (100): -876.61\tReward: -480.22\tNet Worth: 9519.78\tEps: 0.1170\tTrades: 110"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KDD1yz8uxGlp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}