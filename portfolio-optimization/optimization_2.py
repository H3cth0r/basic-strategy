import os
import copy
import time
import random
import numpy as np
import pandas as pd
import yfinance as yf
import networkx as nx
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from scipy.spatial.distance import euclidean
from sklearn.preprocessing import StandardScaler, MinMaxScaler

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import Adam

# Check for PyTorch Geometric
try:
    from torch_geometric.nn import GATConv
    from torch_geometric.data import Data, Batch
except ImportError:
    print("Error: torch_geometric is not installed. Please install it via pip.")
    exit()

# ==========================================
# 1. CONFIGURATION & SETUP
# ==========================================
class Config:
    # Time settings
    START_DATE = "2020-01-01"
    END_DATE = "2023-12-31"
    
    # Split dates (Approximate based on paper)
    TRAIN_END = "2023-04-01"
    VAL_END = "2023-08-01"
    
    # Model Hyperparameters
    SEQ_LENGTH = 10       # Lookback window size for LSTM
    HIDDEN_DIM = 64       # LSTM and GAT hidden dimension
    NUM_HEADS = 2         # GAT heads
    DROPOUT = 0.2
    LEARNING_RATE = 0.001
    EPOCHS = 50           # Reduced for demonstration (Paper uses more)
    BATCH_SIZE = 32
    
    # Graph Construction
    DTW_THRESHOLD = 0.5   # Threshold tau for technical graph
    
    # Portfolio
    TOP_K_STOCKS = 5      # Number of stocks to select for portfolio
    INITIAL_CAPITAL = 10000

    # Tickers: Using a representative subset of S&P 500 for runtime efficiency
    # In a full run, you would use the top 100 by market cap.
    TICKERS = [
        'AAPL', 'MSFT', 'NVDA', 'ADBE', 'CRM',  # Tech
        'JPM', 'BAC', 'V', 'MA',                # Finance
        'JNJ', 'PFE', 'UNH', 'LLY',             # Healthcare
        'PG', 'KO', 'PEP', 'COST',              # Consumer Defensive
        'XOM', 'CVX',                           # Energy
        'TSLA', 'AMZN', 'HD'                    # Consumer Cyclical
    ]

config = Config()
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Running on device: {device}")

# ==========================================
# 2. DATA LOADING & FEATURE ENGINEERING
# ==========================================

def calculate_technical_indicators(df):
    """
    Calculates the 44+ features mentioned in the paper:
    SMA, Bollinger Bands, RSI, Percentage Changes, etc.
    """
    df = df.copy()
    close = df['Close']
    
    # 1. Simple Moving Averages (SMA)
    df['mv9'] = close.rolling(window=9).mean()
    df['mv50'] = close.rolling(window=50).mean()
    df['mv100'] = close.rolling(window=100).mean()
    
    # 2. Bollinger Bands (20-day, 2 std dev)
    df['bb_bbm'] = close.rolling(window=20).mean()
    df['bb_std'] = close.rolling(window=20).std()
    df['bb_bbh'] = df['bb_bbm'] + (2 * df['bb_std'])
    df['bb_bbl'] = df['bb_bbm'] - (2 * df['bb_std'])
    
    # 3. RSI (Relative Strength Index)
    def compute_rsi(series, period):
        delta = series.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
        rs = gain / loss
        return 100 - (100 / (1 + rs))
        
    df['rsi14'] = compute_rsi(close, 14)
    df['rsi50'] = compute_rsi(close, 50)
    df['rsi_mv9'] = df['rsi14'].rolling(window=9).mean() # Derived
    
    # 4. Price Percentage Changes (f1 - f10)
    # Simplified implementation of the logic described
    df['f1'] = (df['Close'] - df['Open']) / df['Open'] * 100
    df['f2'] = (df['High'] - df['Low']) / df['Low'] * 100
    
    for lag in range(1, 5):
        df[f'pct_lag_{lag}'] = df['Close'].pct_change(lag) * 100

    # 5. Moving Average Comparisons & Others
    # Filling missing values generated by rolling windows
    df = df.fillna(method='bfill').fillna(method='ffill')
    
    # Drop intermediate columns if necessary, keeping numeric only
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    return df[numeric_cols]

def get_data(tickers, start, end):
    print("Downloading data...")
    data_dict = {}
    sectors = {}
    
    for t in tickers:
        try:
            # Get History
            ticker_obj = yf.Ticker(t)
            hist = ticker_obj.history(start=start, end=end)
            
            # Get Sector (Fundamental info)
            try:
                info = ticker_obj.info
                sec = info.get('sector', 'Unknown')
            except:
                sec = 'Unknown'
            sectors[t] = sec
            
            if len(hist) > 0:
                # Feature Engineering
                feat_df = calculate_technical_indicators(hist)
                data_dict[t] = feat_df
        except Exception as e:
            print(f"Failed to fetch {t}: {e}")
            
    # Align dates across all stocks (Intersection of dates)
    common_index = None
    for t, df in data_dict.items():
        if common_index is None:
            common_index = df.index
        else:
            common_index = common_index.intersection(df.index)
            
    aligned_data = {}
    for t, df in data_dict.items():
        aligned_data[t] = df.loc[common_index]
        
    print(f"Data processing complete. {len(aligned_data)} tickers aligned with {len(common_index)} time steps.")
    return aligned_data, sectors, common_index

# ==========================================
# 3. GRAPH CONSTRUCTION
# ==========================================

def build_graphs(data_dict, sectors, tickers, threshold=0.5):
    """
    Constructs two adjacency matrices:
    1. Technical Graph (DTW-based)
    2. Fundamental Graph (Sector-based)
    """
    num_nodes = len(tickers)
    
    # Pre-compute normalized close price series for DTW
    # Using simple Euclidean on sequences as a proxy for FastDTW to avoid external C-dependency issues in this script.
    # The paper uses DTW on normalized prices.
    
    price_series = []
    for t in tickers:
        series = data_dict[t]['Close'].values
        # Normalize to [0, 1] for distance calculation
        series_norm = (series - np.min(series)) / (np.max(series) - np.min(series) + 1e-6)
        price_series.append(series_norm)
    
    # --- 1. Technical Graph (DTW/Euclidean proxy) ---
    adj_tech = np.zeros((num_nodes, num_nodes))
    
    print("Constructing Technical Graph (Similarity)...")
    for i in range(num_nodes):
        for j in range(i + 1, num_nodes):
            # Calculate distance (Paper uses FastDTW)
            # dist, _ = fastdtw(price_series[i], price_series[j], radius=1)
            # Using Euclidean on last 100 days to approximate trajectory similarity for speed
            s_i = price_series[i][-100:]
            s_j = price_series[j][-100:]
            if len(s_i) == len(s_j):
                dist = euclidean(s_i, s_j)
                
                # Equation 49: weight = tau - distance
                weight = threshold - (dist / 100) # Scaling distance down roughly
                
                if weight > 0:
                    adj_tech[i, j] = weight
                    adj_tech[j, i] = weight
            
    # --- 2. Fundamental Graph (Sector) ---
    adj_fund = np.zeros((num_nodes, num_nodes))
    print("Constructing Fundamental Graph (Sector)...")
    for i in range(num_nodes):
        for j in range(i + 1, num_nodes):
            if sectors[tickers[i]] == sectors[tickers[j]] and sectors[tickers[i]] != 'Unknown':
                adj_fund[i, j] = 1.0
                adj_fund[j, i] = 1.0
                
    return adj_tech, adj_fund

def graph_to_edge_index(adj_matrix):
    """Convert adjacency matrix to PyG edge_index and edge_weight"""
    rows, cols = np.where(adj_matrix > 0)
    weights = adj_matrix[rows, cols]
    
    edge_index = torch.tensor(np.array([rows, cols]), dtype=torch.long)
    edge_attr = torch.tensor(weights, dtype=torch.float)
    return edge_index, edge_attr

# ==========================================
# 4. DATASET PREPARATION
# ==========================================

class StockGraphDataset:
    def __init__(self, data_dict, tickers, seq_len, train_end, val_end, dates):
        self.data_dict = data_dict
        self.tickers = tickers
        self.seq_len = seq_len
        self.dates = dates
        
        # Determine split indices
        self.train_idx = np.where(dates <= train_end)[0][-1]
        self.val_idx = np.where(dates <= val_end)[0][-1]
        
        # Prepare feature tensors (Scale based on Train)
        self.features = []
        self.targets = []
        
        # Features to use (All columns from engineering)
        sample_df = data_dict[tickers[0]]
        self.feature_cols = sample_df.columns
        self.target_col = 'Close'
        
        # Scaling
        self.scalers = {}
        processed_data = []
        
        # Scale each ticker independently based on training split
        for t in tickers:
            df = data_dict[t]
            raw_vals = df.values
            target_vals = df[self.target_col].values.reshape(-1, 1)
            
            scaler_X = StandardScaler()
            scaler_Y = MinMaxScaler() # Normalize price for prediction
            
            # Fit on train
            train_X = raw_vals[:self.train_idx+1]
            train_Y = target_vals[:self.train_idx+1]
            
            scaler_X.fit(train_X)
            scaler_Y.fit(train_Y)
            
            self.scalers[t] = scaler_Y
            
            # Transform all
            norm_X = scaler_X.transform(raw_vals)
            norm_Y = scaler_Y.transform(target_vals)
            
            processed_data.append((norm_X, norm_Y))
            
        self.processed_data = processed_data # List of (X, Y) per ticker
        
    def get_loader(self, mode='train'):
        if mode == 'train':
            start_i = self.seq_len
            end_i = self.train_idx
        elif mode == 'val':
            start_i = self.train_idx + 1
            end_i = self.val_idx
        else: # test
            start_i = self.val_idx + 1
            end_i = len(self.dates) - 1
            
        X_batch = []
        Y_batch = []
        
        # Create sliding windows
        # Note: For GNN, we process one timestamp (snapshot of all companies) at a time
        # or batches of timestamps. Here we create a list of snapshots.
        
        snapshots = []
        
        for t_step in range(start_i, end_i):
            # For this timestamp, gather data for all nodes
            node_features = []
            node_targets = []
            
            for i, (p_X, p_Y) in enumerate(self.processed_data):
                # Input: Sequence [t-seq_len : t]
                seq_x = p_X[t_step - self.seq_len : t_step]
                # Target: Price at [t] (Next day prediction)
                target_y = p_Y[t_step] 
                
                node_features.append(seq_x)
                node_targets.append(target_y)
            
            # Shape: (Num_Nodes, Seq_Len, Num_Features)
            snapshot_x = torch.tensor(np.array(node_features), dtype=torch.float)
            # Shape: (Num_Nodes, 1)
            snapshot_y = torch.tensor(np.array(node_targets), dtype=torch.float)
            
            snapshots.append((snapshot_x, snapshot_y, self.dates[t_step]))
            
        return snapshots

# ==========================================
# 5. MODEL ARCHITECTURE (BiLSTM-GAT-AM)
# ==========================================

class AttentionFusion(nn.Module):
    """
    Fuses outputs from Technical Graph and Fundamental Graph.
    Eq 53, 54 in the paper.
    """
    def __init__(self, hidden_dim):
        super(AttentionFusion, self).__init__()
        self.w_attn = nn.Linear(hidden_dim * 2, 1) # Learnable vector 'a'
        
    def forward(self, h_tech, h_fund):
        # h_tech: [N, H], h_fund: [N, H]
        
        # Stack: [N, 2, H]
        stacked = torch.stack([h_tech, h_fund], dim=1)
        
        # Concatenate for attention calc (simplified logic matching paper concept)
        # Paper says: exp(a^T * concat(h1, h2))
        
        # Calculate scores
        # We process each channel. 
        # Score 1
        cat_1 = torch.cat([h_tech, h_fund], dim=1) # [N, 2H]
        score_1 = torch.exp(F.leaky_relu(self.w_attn(cat_1)))
        
        # Score 2 (Symmetric or shared? Paper implies single 'a' over concat)
        # Let's use a softmax over the two channels per node
        
        # Alternative implementation for clarity:
        # Transform projected features
        scores = []
        for h in [h_tech, h_fund]:
            # Context vector could be static or dependent
            # Using simple linear projection to scalar
            # A more robust way based on GAT logic:
            s = self.w_attn(torch.cat([h, h_tech + h_fund], dim=1)) # Just combining info
            scores.append(s)
            
        scores = torch.cat(scores, dim=1) # [N, 2]
        weights = F.softmax(scores, dim=1) # [N, 2]
        
        # Weighted Sum
        out = weights[:, 0:1] * h_tech + weights[:, 1:2] * h_fund
        return out, weights

class BiLSTM_GAT_AM(nn.Module):
    def __init__(self, num_nodes, in_features, hidden_dim, num_heads, dropout):
        super(BiLSTM_GAT_AM, self).__init__()
        
        # 1. Bi-LSTM (Temporal Feature Extraction)
        # Input: (Batch=N, Seq, Feat) -> Output: (Batch=N, Hidden*2)
        self.lstm = nn.LSTM(input_size=in_features, 
                            hidden_size=hidden_dim, 
                            num_layers=2, 
                            batch_first=True, 
                            bidirectional=True)
        
        self.lstm_proj = nn.Linear(hidden_dim * 2, hidden_dim) # Project back to H
        
        # 2. GAT Channels
        # Technical Graph Layers (3 layers)
        self.gat_tech_1 = GATConv(hidden_dim, hidden_dim, heads=num_heads, dropout=dropout)
        self.gat_tech_2 = GATConv(hidden_dim * num_heads, hidden_dim, heads=num_heads, dropout=dropout)
        self.gat_tech_3 = GATConv(hidden_dim * num_heads, hidden_dim, heads=1, concat=False, dropout=dropout)
        
        # Fundamental Graph Layers (3 layers)
        self.gat_fund_1 = GATConv(hidden_dim, hidden_dim, heads=num_heads, dropout=dropout)
        self.gat_fund_2 = GATConv(hidden_dim * num_heads, hidden_dim, heads=num_heads, dropout=dropout)
        self.gat_fund_3 = GATConv(hidden_dim * num_heads, hidden_dim, heads=1, concat=False, dropout=dropout)
        
        # 3. Attention Fusion
        self.fusion = AttentionFusion(hidden_dim)
        
        # 4. Final Prediction (MLP)
        self.regressor = nn.Sequential(
            nn.Linear(hidden_dim, 32),
            nn.ReLU(),
            nn.Linear(32, 1) # Predict next Close price (normalized)
        )
        
    def forward(self, x, edge_index_tech, edge_weight_tech, edge_index_fund, edge_weight_fund):
        # x shape: [Num_Nodes, Seq_Len, Features]
        
        # --- LSTM Step ---
        # Pass each node's sequence through LSTM
        # LSTM output: (N, Seq, 2*H)
        out, (hn, cn) = self.lstm(x)
        
        # Take the last hidden state: [2 (bi), N, H] -> reshape/cat
        # hn shape is (num_layers*num_directions, batch, hidden_size)
        # We take the last layer, fwd and bwd
        hn_last = torch.cat((hn[-2], hn[-1]), dim=1) # [N, 2*H]
        
        node_features = self.lstm_proj(hn_last) # [N, H]
        
        # --- GAT Step (Dual Channel) ---
        
        # Channel 1: Technical
        t1 = F.elu(self.gat_tech_1(node_features, edge_index_tech, edge_weight_tech))
        t2 = F.elu(self.gat_tech_2(t1, edge_index_tech, edge_weight_tech))
        t_final = self.gat_tech_3(t2, edge_index_tech, edge_weight_tech)
        
        # Channel 2: Fundamental
        f1 = F.elu(self.gat_fund_1(node_features, edge_index_fund, edge_weight_fund))
        f2 = F.elu(self.gat_fund_2(f1, edge_index_fund, edge_weight_fund))
        f_final = self.gat_fund_3(f2, edge_index_fund, edge_weight_fund)
        
        # --- Fusion Step ---
        h_fused, weights = self.fusion(t_final, f_final)
        
        # --- Prediction Step ---
        out = self.regressor(h_fused)
        
        return out, weights

# ==========================================
# 6. TRAINING & EVALUATION
# ==========================================

def train_model():
    # 1. Prepare Data
    data, sectors, common_index = get_data(config.TICKERS, config.START_DATE, config.END_DATE)
    
    # 2. Build Graphs
    adj_tech, adj_fund = build_graphs(data, sectors, config.TICKERS, config.DTW_THRESHOLD)
    
    ei_tech, ew_tech = graph_to_edge_index(adj_tech)
    ei_fund, ew_fund = graph_to_edge_index(adj_fund)
    
    # Move to device
    ei_tech, ew_tech = ei_tech.to(device), ew_tech.to(device)
    ei_fund, ew_fund = ei_fund.to(device), ew_fund.to(device)
    
    # 3. Create Dataset
    dataset = StockGraphDataset(data, config.TICKERS, config.SEQ_LENGTH, 
                                config.TRAIN_END, config.VAL_END, common_index)
    
    train_loader = dataset.get_loader('train')
    val_loader = dataset.get_loader('val')
    test_loader = dataset.get_loader('test')
    
    # 4. Initialize Model
    num_features = dataset.features[0].shape[1] if dataset.features else data[config.TICKERS[0]].shape[1]
    
    model = BiLSTM_GAT_AM(len(config.TICKERS), num_features, config.HIDDEN_DIM, config.NUM_HEADS, config.DROPOUT)
    model = model.to(device)
    
    optimizer = Adam(model.parameters(), lr=config.LEARNING_RATE)
    criterion = nn.MSELoss()
    
    # 5. Training Loop
    print("\nStarting Training...")
    loss_history = []
    best_val_loss = float('inf')
    best_model_state = None
    
    for epoch in range(config.EPOCHS):
        model.train()
        total_loss = 0
        
        # Random shuffle batches
        random.shuffle(train_loader)
        
        for batch_x, batch_y, _ in train_loader:
            batch_x = batch_x.to(device)
            batch_y = batch_y.to(device)
            
            optimizer.zero_grad()
            
            preds, _ = model(batch_x, ei_tech, ew_tech, ei_fund, ew_fund)
            
            loss = criterion(preds, batch_y)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            
        avg_loss = total_loss / len(train_loader)
        loss_history.append(avg_loss)
        
        # Validation
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for bx, by, _ in val_loader:
                bx, by = bx.to(device), by.to(device)
                preds, _ = model(bx, ei_tech, ew_tech, ei_fund, ew_fund)
                val_loss += criterion(preds, by).item()
        val_loss /= len(val_loader)
        
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_model_state = copy.deepcopy(model.state_dict())
            
        if (epoch+1) % 5 == 0:
            print(f"Epoch {epoch+1}/{config.EPOCHS} | Train Loss: {avg_loss:.6f} | Val Loss: {val_loss:.6f}")
            
    # Load best model
    model.load_state_dict(best_model_state)
    
    # ==========================================
    # 7. BACKTESTING & VISUALIZATION
    # ==========================================
    print("\nRunning Backtest on Test Set...")
    
    model.eval()
    portfolio_balance = [config.INITIAL_CAPITAL]
    benchmark_balance = [config.INITIAL_CAPITAL]
    dates_test = []
    
    # For allocation pie chart (last day)
    final_allocation = {}
    
    predictions_map = {t: [] for t in config.TICKERS}
    actuals_map = {t: [] for t in config.TICKERS}
    
    # Iterate through test days
    for i, (bx, by, date) in enumerate(test_loader):
        bx = bx.to(device)
        
        # Predict Next Day Price (Normalized)
        with torch.no_grad():
            preds_norm, attn_weights = model(bx, ei_tech, ew_tech, ei_fund, ew_fund)
            
        preds_norm = preds_norm.cpu().numpy().flatten()
        
        # Inverse Transform Predictions
        current_prices = []
        predicted_next_prices = []
        
        # We need the ACTUAL current price (t) to calculate predicted return for t+1
        # bx contains [t-seq : t]. The last element of bx is 't-1' relative to prediction target 't'?
        # In our loader: seq_x is [t-seq : t], target is t.
        # So current known price is the last element of input sequence.
        
        day_returns_pred = []
        
        for idx, t in enumerate(config.TICKERS):
            # Get scaler
            scaler = dataset.scalers[t]
            
            # Inverse transform target
            p_val = preds_norm[idx]
            p_real = scaler.inverse_transform([[p_val]])[0][0]
            
            # Get current price (Last value in input sequence)
            # Input seq was normalized.
            last_input_norm = bx[idx, -1, 0].item() # Assuming Close is at index 0?
            # Actually, we need to know which feature is Close.
            # In calculate_technical_indicators, 'Close' is likely not index 0 because of reordering.
            # Simpler approach: Look up the price in the raw DataFrame using the date.
            
            # Use raw data for logic to be safe
            raw_close = dataset.data_dict[t].loc[date]['Close']
            
            # Predicted Return = (Pred_Price_Next - Curr_Price) / Curr_Price
            pred_ret = (p_real - raw_close) / raw_close
            
            day_returns_pred.append((t, pred_ret, raw_close))
            
            predictions_map[t].append(p_real)
            
            # Get Actual Next Day Price (Target)
            y_norm = by[idx].item()
            y_real = scaler.inverse_transform([[y_norm]])[0][0]
            actuals_map[t].append(y_real)

        # --- Portfolio Strategy (Top K) ---
        # Sort by predicted return
        day_returns_pred.sort(key=lambda x: x[1], reverse=True)
        
        # Select Top K
        top_k = day_returns_pred[:config.TOP_K_STOCKS]
        
        # Calculate Real Portfolio Return for this day
        # We invested at 'raw_close' and sold at 'y_real' (Next day actual)
        daily_pnl = 0
        allocation_per_stock = portfolio_balance[-1] / len(top_k)
        
        current_allocation = {}
        
        for ticker, pred_r, entry_price in top_k:
            # Find actual exit price
            actual_exit = actuals_map[ticker][-1]
            actual_return = (actual_exit - entry_price) / entry_price
            
            daily_pnl += allocation_per_stock * actual_return
            current_allocation[ticker] = allocation_per_stock
            
        new_balance = portfolio_balance[-1] + daily_pnl
        portfolio_balance.append(new_balance)
        
        # Benchmark (S&P 500 equivalent - Equal weight all tickers)
        bench_ret = 0
        for idx, t in enumerate(config.TICKERS):
            actual_exit = actuals_map[t][-1]
            entry_price = day_returns_pred[idx][2] if day_returns_pred[idx][0] == t else 0 
            # (Need to find entry price for all tickers, efficient way below)
            # Re-lookup for benchmark loop simplicity
            entry_price = dataset.data_dict[t].loc[date]['Close']
            bench_ret += (actual_exit - entry_price) / entry_price
            
        bench_ret /= len(config.TICKERS)
        benchmark_balance.append(benchmark_balance[-1] * (1 + bench_ret))
        
        dates_test.append(date)
        
        if i == len(test_loader) - 1:
            final_allocation = current_allocation

    # ==========================================
    # 8. PLOTTING
    # ==========================================
    print("Generating Plots...")
    
    # Plot 1: Portfolio Performance vs Benchmark
    plt.figure(figsize=(12, 6))
    plt.plot(portfolio_balance, label='BiLSTM-GAT-AM Portfolio', linewidth=2)
    plt.plot(benchmark_balance, label='Equal Weight Benchmark', linestyle='--')
    plt.title('Portfolio Cumulative Return Backtest')
    plt.xlabel('Trading Days (Test Set)')
    plt.ylabel('Portfolio Value ($)')
    plt.legend()
    plt.grid(True)
    plt.show()
    
    # Plot 2: Allocation Pie Chart (Final Day)
    plt.figure(figsize=(8, 8))
    labels = list(final_allocation.keys())
    sizes = [1 for _ in labels] # Equal weight among top K
    plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140)
    plt.title(f'Portfolio Allocation (Top {config.TOP_K_STOCKS}) on Last Day')
    plt.show()
    
    # Plot 3: Network Graph Visualization (Technical Similiarity)
    # Visualizing the last computed attention weights or the static structure
    G = nx.Graph()
    # Add nodes
    for i, t in enumerate(config.TICKERS):
        G.add_node(t, sector=sectors[t])
    
    # Add edges from Technical Matrix (Thresholded for visibility)
    rows, cols = np.where(adj_tech > config.DTW_THRESHOLD + 0.1) # Higher thresh for cleaner plot
    for r, c in zip(rows, cols):
        G.add_edge(config.TICKERS[r], config.TICKERS[c], weight=adj_tech[r, c])
        
    pos = nx.spring_layout(G, k=0.3)
    plt.figure(figsize=(12, 12))
    
    # Color by sector
    unique_sectors = list(set(sectors.values()))
    colors = [unique_sectors.index(sectors[n]) for n in G.nodes()]
    
    nx.draw(G, pos, with_labels=True, node_color=colors, cmap=plt.cm.Set3, 
            node_size=1500, font_size=9, font_weight='bold')
    plt.title('Stock Interaction Graph (Technical Similarity)')
    plt.show()
    
    # Plot 4: Prediction vs Actual (First Stock in list)
    t_ex = config.TICKERS[0]
    plt.figure(figsize=(12, 6))
    plt.plot(actuals_map[t_ex], label='Actual Price')
    plt.plot(predictions_map[t_ex], label='Predicted Price')
    plt.title(f'Price Prediction: {t_ex}')
    plt.legend()
    plt.show()

    # Calculate Metrics
    final_ret = (portfolio_balance[-1] - config.INITIAL_CAPITAL) / config.INITIAL_CAPITAL
    print(f"\nFinal Portfolio Balance: ${portfolio_balance[-1]:.2f}")
    print(f"Total Return: {final_ret*100:.2f}%")
    
    # Sharpe Ratio
    returns_series = pd.Series(portfolio_balance).pct_change().dropna()
    sharpe = returns_series.mean() / returns_series.std() * np.sqrt(252)
    print(f"Sharpe Ratio: {sharpe:.4f}")

if __name__ == "__main__":
    train_model()
