

import marimo

__generated_with = "0.13.2"
app = marimo.App(width="medium", auto_download=["html"])


@app.cell
def _():
    import yfinance as yf
    import pandas as pd
    import numpy as np
    from ta import add_all_ta_features
    from sklearn.preprocessing import MinMaxScaler
    from sklearn.model_selection import train_test_split # Although we'll do a time-based split

    import torch
    import torch.nn as nn
    from torch.utils.data import DataLoader, Dataset, TensorDataset

    import matplotlib.pyplot as plt
    import copy # For deep copying the best model

    # --- Configuration ---
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {DEVICE}")
    return (
        DEVICE,
        DataLoader,
        MinMaxScaler,
        TensorDataset,
        add_all_ta_features,
        copy,
        nn,
        np,
        pd,
        plt,
        torch,
        yf,
    )


@app.cell
def _(pd, yf):
    def get_stock_data(ticker: str, period: str = "5y", interval: str = "1d") -> pd.DataFrame:
        """
        Fetches historical stock data using yfinance.

        Args:
            ticker (str): Stock ticker symbol (e.g., 'AAPL').
            period (str): Data period (e.g., '1y', '5y', 'max').
            interval (str): Data interval (e.g., '1d', '1wk', '1mo').

        Returns:
            pd.DataFrame: DataFrame with historical stock data (OHLCV).
                          Returns empty DataFrame if fetching fails.
        """
        print(f"Fetching data for {ticker}...")
        try:
            stock = yf.Ticker(ticker)
            data = stock.history(period=period, interval=interval, auto_adjust=True)
            if data.empty:
                print(f"Warning: No data found for ticker {ticker} with period={period}, interval={interval}")
                return pd.DataFrame()
            # Ensure column names are lowercase for consistency
            data.columns = [col.lower().replace(' ', '_') for col in data.columns]
            print(f"Data fetched successfully. Shape: {data.shape}")
            return data
        except Exception as e:
            print(f"Error fetching data for {ticker}: {e}")
            return pd.DataFrame()
    return (get_stock_data,)


@app.cell
def _(add_all_ta_features, np, pd):
    def calculate_technical_indicators(data: pd.DataFrame) -> pd.DataFrame:
        """
        Calculates technical indicators using the 'ta' library.

        Args:
            data (pd.DataFrame): DataFrame with OHLCV data. Must contain
                                 'open', 'high', 'low', 'close', 'volume' columns.

        Returns:
            pd.DataFrame: Original DataFrame augmented with technical indicator features.
                          Handles NaNs introduced by indicators using fillna=True.
                          It might still contain NaNs if the input data had them.
                          Further NaN handling (like after lagging) might be needed.
        """
        if data.empty:
            print("Skipping technical indicators calculation: Input data is empty.")
            return data
        required_cols = ['open', 'high', 'low', 'close', 'volume']
        if not all(col in data.columns for col in required_cols):
            missing_cols = [col for col in required_cols if col not in data.columns]
            print(f"Warning: Missing required OHLCV columns for TA calculation: {missing_cols}. Skipping.")
            # Optionally, return data or raise an error depending on desired behavior
            # raise ValueError(f"Missing required columns for TA: {missing_cols}")
            return data # Returning original data if columns missing

        print("Calculating technical indicators (using fillna=True)...")
        try:
            # Store original columns to identify added ones
            original_columns = set(data.columns)

            # Calculate indicators using fillna=True
            data_ta = add_all_ta_features(
                data.copy(), # Use a copy
                open="open",
                high="high",
                low="low",
                close="close",
                volume="volume",
                fillna=True # <--- CHANGE HERE: Use internal filling provided by 'ta'
            )

            # Identify columns added by 'ta'
            added_ta_columns = list(set(data_ta.columns) - original_columns)

            # Clean up potential infinite values sometimes generated by indicators
            # Identify numeric columns among the added TA columns + original required cols
            numeric_cols_to_check = list(original_columns.intersection(required_cols)) + added_ta_columns
            numeric_cols_to_check = data_ta[numeric_cols_to_check].select_dtypes(include=np.number).columns.tolist()

            if numeric_cols_to_check:
                count_infinite_before = data_ta[numeric_cols_to_check].isin([np.inf, -np.inf]).sum().sum()
                if count_infinite_before > 0:
                    print(f"Replacing {count_infinite_before} infinite values with NaN...")
                    data_ta.replace([np.inf, -np.inf], np.nan, inplace=True)
                    # Re-apply fillna after replacing inf -> NaN for indicator columns
                    # Using forward fill and then backward fill is a common strategy
                    print(f"Applying ffill/bfill to handle NaNs created from infinite values or missed by ta's fillna...")
                    # Limit filling to only numeric TA columns to avoid issues with non-numeric ones if any exist
                    cols_to_fill = data_ta[added_ta_columns].select_dtypes(include=np.number).columns.tolist()
                    data_ta[cols_to_fill] = data_ta[cols_to_fill].ffill().bfill()

            # Optional: Drop rows that *still* have NaNs after ta's fillna and our inf handling
            # This might happen if the *original* data had NaNs or if ffill/bfill couldn't fill everything
            original_rows = data_ta.shape[0]
            data_ta.dropna(inplace=True) # Now drop any *remaining* NaNs
            dropped_rows = original_rows - data_ta.shape[0]
            if dropped_rows > 0:
                 print(f"Dropped {dropped_rows} rows containing remaining NaNs after indicator calculation and filling.")

            print(f"Technical indicators calculated. Shape after processing: {data_ta.shape}")
            if data_ta.empty:
                print("Warning: Data is empty even after using fillna=True and processing NaNs. Check original data quality.")

            return data_ta
        except Exception as e:
            print(f"Error calculating technical indicators: {e}")
            # Return original data if TA fails
            return data

    def create_lagged_features(data: pd.DataFrame, features_to_lag: list, lag_periods: list) -> pd.DataFrame:
        """
        Creates lagged versions of specified features.

        Args:
            data (pd.DataFrame): Input DataFrame.
            features_to_lag (list): List of column names to create lags for.
            lag_periods (list): List of integers representing the lag steps (e.g., [1, 2, 3]).

        Returns:
            pd.DataFrame: DataFrame with original and lagged features.
                          Drops rows with NaNs introduced by lagging.
        """
        if data.empty:
            print("Skipping lagged feature creation: Input data is empty.")
            return data

        print(f"Creating lagged features for {features_to_lag} with lags {lag_periods}...")
        data_lagged = data.copy()
        for feature in features_to_lag:
            if feature in data_lagged.columns:
                for lag in lag_periods:
                    data_lagged[f'{feature}_lag_{lag}'] = data_lagged[feature].shift(lag)
            else:
                print(f"Warning: Feature '{feature}' not found in data. Skipping lag creation for it.")

        # Drop rows with NaNs created by shifting (at the beginning of the series)
        original_rows = data_lagged.shape[0]
        data_lagged.dropna(inplace=True)
        print(f"Lagged features created. Shape after dropping NaNs: {data_lagged.shape} (dropped {original_rows - data_lagged.shape[0]} rows)")
        return data_lagged
    return calculate_technical_indicators, create_lagged_features


@app.cell
def _(MinMaxScaler, np, pd):
    def preprocess_data(data: pd.DataFrame,
                        target_column: str,
                        sequence_length: int,
                        n_outputs: int, # <--- Added: Number of steps to predict
                        test_split_ratio: float):
        """
        Preprocesses data for LSTM: scales features, splits into train/test,
        and creates sequences for multi-step prediction.

        Args:
            data (pd.DataFrame): DataFrame with all features (original, TA, lagged).
            target_column (str): The name of the column to predict.
            sequence_length (int): The number of time steps in each input sequence.
            n_outputs (int): The number of future time steps to predict.
            test_split_ratio (float): Fraction of data to use for the test set.

        Returns:
            tuple: Contains:
                - X_train (np.array): Training features (sequences).
                - y_train (np.array): Training target (sequences of n_outputs).
                - X_test (np.array): Testing features (sequences).
                - y_test (np.array): Testing target (sequences of n_outputs).
                - scaler (MinMaxScaler): Fitted scaler object for inverse transformation.
                - feature_names (list): List of feature column names used.
                - target_col_index (int): Index of the target column in the scaled data.
        """
        if data.empty:
            print("Skipping preprocessing: Input data is empty.")
            return None, None, None, None, None, None, None
        if target_column not in data.columns:
             raise ValueError(f"Target column '{target_column}' not found in DataFrame columns: {data.columns.tolist()}")

        print(f"Preprocessing data for {n_outputs}-step prediction...")

        data = data.sort_index()
        feature_names = data.select_dtypes(include=np.number).columns.tolist()
        if target_column not in feature_names:
             raise ValueError(f"Target column '{target_column}' is not numeric or not found after TA/lags.")

        try:
            target_col_index = feature_names.index(target_column)
            print(f"Target column '{target_column}' found at index {target_col_index}.")
        except ValueError:
             raise ValueError(f"Target column '{target_column}' not found in the final numeric feature list: {feature_names}")

        scaler = MinMaxScaler()
        scaled_data = scaler.fit_transform(data[feature_names].values)

        split_index = int(len(scaled_data) * (1 - test_split_ratio))
        # Ensure enough data for the last sequence and its target horizon in training
        train_data = scaled_data[:split_index]
        # Overlap needed for first test sequence, ensure enough data for last test sequence + horizon
        test_data = scaled_data[split_index - sequence_length:]

        print(f"Data split: Train shape={train_data.shape}, Test shape={test_data.shape}")

        # --- Create sequences for multi-step ---
        def create_sequences_multi_step(input_data, seq_len, num_outputs, target_idx):
            """Helper to create sequences and corresponding multi-step targets."""
            X, y = [], []
            # Adjust loop range to ensure targets for 'num_outputs' steps exist
            for i in range(len(input_data) - seq_len - num_outputs + 1):
                X.append(input_data[i:(i + seq_len)])
                # Target is the sequence of 'num_outputs' values AFTER the input sequence
                y.append(input_data[i + seq_len : i + seq_len + num_outputs, target_idx])
            return np.array(X), np.array(y)

        X_train, y_train = create_sequences_multi_step(train_data, sequence_length, n_outputs, target_col_index)
        X_test, y_test = create_sequences_multi_step(test_data, sequence_length, n_outputs, target_col_index)

        # y_train/y_test are now shape (num_samples, n_outputs) - no reshape needed here

        print(f"Sequences created: X_train shape={X_train.shape}, y_train shape={y_train.shape}")
        print(f"Sequences created: X_test shape={X_test.shape}, y_test shape={y_test.shape}")

        # Check if enough test data was generated
        if X_test.shape[0] == 0:
            print("\n !!! WARNING !!!")
            print("No test sequences were generated. This might be because:")
            print(f"1. The test split ({test_split_ratio}) results in too few data points.")
            print(f"2. The remaining data points ({len(test_data)}) are less than sequence_length ({sequence_length}) + n_outputs ({n_outputs}).")
            print("Consider reducing test_split_ratio, sequence_length, or n_outputs, or using more data.")
            print("Pipeline might fail in subsequent steps.")
            print(" !!! WARNING !!!\n")


        return X_train, y_train, X_test, y_test, scaler, feature_names, target_col_index
    return (preprocess_data,)


@app.cell
def _(DataLoader, TensorDataset, nn, torch):
    class LSTMModel(nn.Module):
        """
        LSTM Network Architecture.
        """
        def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_prob=0.2):
            """
            Args:
                input_size (int): Number of features in the input.
                hidden_size (int): Number of features in the hidden state h.
                num_layers (int): Number of recurrent layers.
                output_size (int): Number of output features (usually 1 for regression).
                dropout_prob (float): Dropout probability.
            """
            super(LSTMModel, self).__init__()
            self.hidden_size = hidden_size
            self.num_layers = num_layers

            self.lstm = nn.LSTM(input_size, hidden_size, num_layers,
                                batch_first=True, # Input shape: (batch, seq_len, features)
                                dropout=dropout_prob if num_layers > 1 else 0) # Dropout between LSTM layers

            self.dropout = nn.Dropout(dropout_prob) # Dropout before final layer
            self.fc = nn.Linear(hidden_size, output_size)

        def forward(self, x):
            """
            Forward pass through the network.

            Args:
                x (torch.Tensor): Input tensor of shape (batch_size, sequence_length, input_size).

            Returns:
                torch.Tensor: Output tensor of shape (batch_size, output_size).
            """
            # Initialize hidden and cell states
            h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
            c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)

            # We need to detach the hidden state to prevent backpropagating through the entire history
            out, _ = self.lstm(x, (h0.detach(), c0.detach()))

            # We only need the output of the last time step for prediction
            out = out[:, -1, :]

            # Apply dropout and the final fully connected layer
            out = self.dropout(out)
            out = self.fc(out)
            return out

    # Helper function to create DataLoaders
    def create_dataloaders(X_train, y_train, X_test, y_test, batch_size):
        """Creates PyTorch DataLoaders for training and testing."""
        print(f"Creating DataLoaders with batch size {batch_size}...")

        # Convert numpy arrays to PyTorch tensors
        train_features = torch.tensor(X_train, dtype=torch.float32)
        train_targets = torch.tensor(y_train, dtype=torch.float32)
        test_features = torch.tensor(X_test, dtype=torch.float32)
        test_targets = torch.tensor(y_test, dtype=torch.float32)

        # Create TensorDatasets
        train_dataset = TensorDataset(train_features, train_targets)
        test_dataset = TensorDataset(test_features, test_targets)

        # Create DataLoaders
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True) # Shuffle training data
        # Don't shuffle test data if you want to plot predictions chronologically
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False)

        print("DataLoaders created.")
        return train_loader, test_loader
    return LSTMModel, create_dataloaders


@app.cell
def _(DEVICE, DataLoader, copy, nn, torch):
    def train_model(model: nn.Module,
                    train_loader: DataLoader,
                    val_loader: DataLoader,
                    criterion,
                    optimizer,
                    num_epochs: int,
                    patience: int = 10):
        """
        Trains the LSTM model.

        Args:
            model (nn.Module): The LSTM model instance.
            train_loader (DataLoader): DataLoader for training data.
            val_loader (DataLoader): DataLoader for validation data.
            criterion: Loss function (e.g., nn.MSELoss()).
            optimizer: Optimizer (e.g., torch.optim.Adam).
            num_epochs (int): Number of training epochs.
            patience (int): Number of epochs to wait for improvement before early stopping.

        Returns:
            tuple: Contains:
                - nn.Module: The trained model (best validation performance).
                - list: Training loss history per epoch.
                - list: Validation loss history per epoch.
        """
        print(f"Starting training for {num_epochs} epochs...")
        model.to(DEVICE)
        train_loss_history = []
        val_loss_history = []
        best_val_loss = float('inf')
        epochs_no_improve = 0
        best_model_wts = copy.deepcopy(model.state_dict()) # Store best model weights

        for epoch in range(num_epochs):
            model.train()  # Set model to training mode
            running_train_loss = 0.0

            for batch_idx, (features, targets) in enumerate(train_loader):
                features, targets = features.to(DEVICE), targets.to(DEVICE)

                # Forward pass
                outputs = model(features)
                loss = criterion(outputs, targets)

                # Backward pass and optimize
                optimizer.zero_grad()
                loss.backward()
                # Optional: Gradient Clipping
                # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()

                running_train_loss += loss.item() * features.size(0) # loss.item() is avg loss per batch item

            epoch_train_loss = running_train_loss / len(train_loader.dataset)
            train_loss_history.append(epoch_train_loss)

            # --- Validation Phase ---
            model.eval()  # Set model to evaluation mode
            running_val_loss = 0.0
            with torch.no_grad(): # No need to track gradients during validation
                for features, targets in val_loader:
                    features, targets = features.to(DEVICE), targets.to(DEVICE)
                    outputs = model(features)
                    loss = criterion(outputs, targets)
                    running_val_loss += loss.item() * features.size(0)

            epoch_val_loss = running_val_loss / len(val_loader.dataset)
            val_loss_history.append(epoch_val_loss)

            print(f"Epoch {epoch+1}/{num_epochs} - Train Loss: {epoch_train_loss:.6f}, Val Loss: {epoch_val_loss:.6f}")

            # --- Early Stopping Check ---
            if epoch_val_loss < best_val_loss:
                best_val_loss = epoch_val_loss
                best_model_wts = copy.deepcopy(model.state_dict())
                epochs_no_improve = 0
                print(f"Validation loss improved to {best_val_loss:.6f}. Saving model...")
            else:
                epochs_no_improve += 1
                print(f"Validation loss did not improve for {epochs_no_improve} epoch(s).")

            if epochs_no_improve >= patience:
                print(f"Early stopping triggered after {epoch + 1} epochs.")
                break

        print("Training finished.")
        # Load best model weights
        model.load_state_dict(best_model_wts)
        return model, train_loss_history, val_loss_history
    return (train_model,)


@app.cell
def _(DEVICE, DataLoader, MinMaxScaler, nn, np, plt, torch):
    def evaluate_model(model: nn.Module, data_loader: DataLoader, criterion):
        """
        Evaluates the model on a given dataset.

        Args:
            model (nn.Module): Trained model.
            data_loader (DataLoader): DataLoader for the evaluation data (e.g., test set).
            criterion: Loss function used during training.

        Returns:
            float: Average loss on the evaluation dataset.
        """
        print("Evaluating model...")
        model.eval() # Set model to evaluation mode
        model.to(DEVICE)
        running_loss = 0.0
        with torch.no_grad():
            for features, targets in data_loader:
                features, targets = features.to(DEVICE), targets.to(DEVICE)
                outputs = model(features)
                loss = criterion(outputs, targets)
                running_loss += loss.item() * features.size(0)

        avg_loss = running_loss / len(data_loader.dataset)
        print(f"Evaluation Loss: {avg_loss:.6f}")
        return avg_loss

    from sklearn.metrics import mean_squared_error # Import needed
    def make_predictions(model: nn.Module,
                           data_loader: DataLoader,
                           scaler: MinMaxScaler,
                           n_features: int,
                           target_col_index: int,
                           n_outputs: int): # <--- Added
        """
        Makes multi-step predictions using the trained model and inverse transforms them.

        Args:
            model (nn.Module): Trained model.
            data_loader (DataLoader): DataLoader for the data to predict on (usually test set).
            scaler (MinMaxScaler): Scaler object used during preprocessing.
            n_features (int): The total number of features the scaler was fit on.
            target_col_index (int): The index of the target column within the scaled data.
            n_outputs (int): The number of prediction steps (horizon).

        Returns:
            tuple: Contains:
                - np.array: Actual target values (inverse transformed), shape (num_samples, n_outputs).
                - np.array: Predicted target values (inverse transformed), shape (num_samples, n_outputs).
        """
        print(f"Making {n_outputs}-step predictions...")
        model.eval()
        model.to(DEVICE)
        all_preds_scaled = []
        all_actuals_scaled = []

        with torch.no_grad():
            for features, targets in data_loader:
                # features shape: (batch_size, seq_len, n_features)
                # targets shape: (batch_size, n_outputs)
                features = features.to(DEVICE)
                outputs = model(features) # outputs shape: (batch_size, n_outputs)
                all_preds_scaled.append(outputs.cpu().numpy())
                all_actuals_scaled.append(targets.cpu().numpy()) # targets are already cpu

        # Concatenate predictions and actuals from all batches
        all_preds_scaled = np.concatenate(all_preds_scaled, axis=0) # Shape: (total_samples, n_outputs)
        all_actuals_scaled = np.concatenate(all_actuals_scaled, axis=0) # Shape: (total_samples, n_outputs)

        # --- Inverse Transform ---
        # We need to inverse transform each step individually because the scaler
        # expects the shape (n_samples, n_features)
        actuals_inv = np.zeros_like(all_actuals_scaled)
        preds_inv = np.zeros_like(all_preds_scaled)

        for i in range(n_outputs):
            # Create dummy array for the i-th step prediction
            dummy_preds_step_i = np.zeros((len(all_preds_scaled), n_features))
            dummy_preds_step_i[:, target_col_index] = all_preds_scaled[:, i]
            preds_inv[:, i] = scaler.inverse_transform(dummy_preds_step_i)[:, target_col_index]

            # Create dummy array for the i-th step actual value
            dummy_actuals_step_i = np.zeros((len(all_actuals_scaled), n_features))
            dummy_actuals_step_i[:, target_col_index] = all_actuals_scaled[:, i]
            actuals_inv[:, i] = scaler.inverse_transform(dummy_actuals_step_i)[:, target_col_index]

        print("Predictions made and inverse transformed.")
        return actuals_inv, preds_inv # Shape: (total_samples, n_outputs)

    def plot_results(actuals: np.array, predictions: np.array, title: str = "Stock Price Prediction"):
        """Plots actual vs. predicted values."""
        plt.figure(figsize=(14, 7))
        plt.plot(actuals, label='Actual Values', color='blue', alpha=0.7)
        plt.plot(predictions, label='Predicted Values', color='red', linestyle='--', alpha=0.7)
        plt.title(title)
        plt.xlabel("Time Steps (Test Set)")
        plt.ylabel("Stock Price")
        plt.legend()
        plt.grid(True)
        plt.show()

    def plot_loss(train_loss: list, val_loss: list):
        """Plots training and validation loss curves."""
        plt.figure(figsize=(10, 5))
        plt.plot(train_loss, label='Training Loss')
        plt.plot(val_loss, label='Validation Loss')
        plt.title("Model Loss During Training")
        plt.xlabel("Epoch")
        plt.ylabel("Loss (MSE)")
        plt.legend()
        plt.grid(True)
        plt.show()
    return evaluate_model, make_predictions, mean_squared_error, plot_loss


@app.cell
def _(mean_squared_error, np):
    def calculate_step_errors(actuals: np.array, predictions: np.array, n_outputs: int):
        """
        Calculates the Root Mean Squared Error (RMSE) for each prediction step.

        Args:
            actuals (np.array): Inverse-transformed actual values (shape: num_samples, n_outputs).
            predictions (np.array): Inverse-transformed predicted values (shape: num_samples, n_outputs).
            n_outputs (int): Number of prediction steps.

        Returns:
            dict: A dictionary where keys are step numbers (1 to n_outputs)
                  and values are the RMSE for that step.
        """
        print("Calculating error per prediction step...")
        step_rmse = {}
        if actuals.shape[1] != n_outputs or predictions.shape[1] != n_outputs:
            raise ValueError(f"Input arrays must have {n_outputs} columns (steps).")

        for i in range(n_outputs):
            step_actuals = actuals[:, i]
            step_preds = predictions[:, i]
            rmse = np.sqrt(mean_squared_error(step_actuals, step_preds))
            step_rmse[f'Step_{i+1}_RMSE'] = rmse
            print(f"  Step {i+1} RMSE: {rmse:.4f}")

        return step_rmse
    return (calculate_step_errors,)


@app.cell
def _(np, plt):
    def plot_first_step_results(actuals: np.array, predictions: np.array, title: str = "Stock Price Prediction (1st Step)"):
        """Plots actual vs. predicted values for the FIRST prediction step."""
        plt.figure(figsize=(14, 7))
        # actuals and predictions have shape (num_samples, n_outputs)
        plt.plot(actuals[:, 0], label='Actual Values (Step 1)', color='blue', alpha=0.8)
        plt.plot(predictions[:, 0], label='Predicted Values (Step 1)', color='red', linestyle='--', alpha=0.7)
        plt.title(title)
        plt.xlabel("Time Steps (Test Set Samples)")
        plt.ylabel("Stock Price")
        plt.legend()
        plt.grid(True)
        plt.show()

    def plot_error_by_step(step_errors: dict):
        """Plots the calculated error (e.g., RMSE) for each prediction step."""
        steps = list(step_errors.keys())
        errors = list(step_errors.values())

        plt.figure(figsize=(8, 5))
        plt.bar(steps, errors, color='skyblue')
        plt.xlabel("Prediction Step")
        plt.ylabel("RMSE")
        plt.title("Error (RMSE) by Prediction Horizon")
        plt.grid(axis='y', linestyle='--', alpha=0.7)
        # Add error values on top of bars
        for i, v in enumerate(errors):
            plt.text(i, v + (max(errors)*0.01), f"{v:.3f}", ha='center', va='bottom')
        plt.show()

    def plot_multi_step_forecast_examples(actuals: np.array, predictions: np.array, n_outputs: int, num_examples: int = 5, title: str = "Multi-Step Forecast Examples"):
        """Plots a few examples of multi-step forecasts vs actuals."""
        plt.figure(figsize=(15, num_examples * 3)) # Adjust figure size
        num_samples = actuals.shape[0]
        step_indices = np.arange(n_outputs)

        # Ensure we don't pick indices too close to the end for plotting actuals
        plot_indices = np.linspace(0, num_samples - n_outputs - 1, num_examples, dtype=int)

        for i, start_idx in enumerate(plot_indices):
            plt.subplot(num_examples, 1, i + 1)
            # Plot actual future path for this example
            actual_future = actuals[start_idx, :] # These are the actual values corresponding to the prediction steps
            # We need the history *leading up to* the prediction point for context,
            # let's plot the last 'n_outputs' actual points before the forecast starts
            history_end_idx = start_idx
            history_start_idx = max(0, history_end_idx - n_outputs)
            history_actuals = actuals[history_start_idx:history_end_idx, 0] # Use first column (step 1 actuals) for history
            history_indices = np.arange(history_start_idx, history_end_idx) - history_end_idx # Relative indices <= 0

            # Plot history
            if len(history_actuals) > 0:
                 plt.plot(history_indices, history_actuals, 'ko-', label='History (Actual Step 1)', markersize=3)

            # Plot actual future values (n_outputs steps)
            plt.plot(step_indices, actual_future, 'bo-', label=f'Actual Future (Steps 1-{n_outputs})', markersize=3)

            # Plot the predicted future values (n_outputs steps)
            forecast = predictions[start_idx, :]
            plt.plot(step_indices, forecast, 'ro--', label=f'Forecast (Steps 1-{n_outputs})', markersize=3)

            plt.title(f"Forecast starting at Test Sample Index {start_idx}")
            plt.xlabel("Steps Ahead (0 = Prediction Point)")
            plt.ylabel("Price")
            plt.legend(loc='best')
            plt.grid(True)

        plt.suptitle(title, y=1.02) # Add overall title
        plt.tight_layout()
        plt.show()
    return (
        plot_error_by_step,
        plot_first_step_results,
        plot_multi_step_forecast_examples,
    )


@app.cell
def _(
    LSTMModel,
    calculate_step_errors,
    calculate_technical_indicators,
    create_dataloaders,
    create_lagged_features,
    evaluate_model,
    get_stock_data,
    make_predictions,
    nn,
    np,
    plot_error_by_step,
    plot_first_step_results,
    plot_loss,
    plot_multi_step_forecast_examples,
    preprocess_data,
    torch,
    train_model,
):
    def stock_prediction_pipeline(
        # Data params
        ticker: str = 'AAPL',
        period: str = '5y',
        interval: str = '1d',
        # Feature params
        features_to_lag: list = ['close', 'volume'],
        lag_periods: list = [1, 3, 5],
        target_column: str = 'close',
        # Preprocessing params
        sequence_length: int = 60,
        n_outputs: int = 5,                          # <--- Added: Predict 5 steps
        test_split_ratio: float = 0.2,
        # Model params
        hidden_size: int = 100,
        num_layers: int = 2,
        dropout_prob: float = 0.2,
        # Training params
        learning_rate: float = 0.001,
        num_epochs: int = 50,
        batch_size: int = 64,
        patience: int = 10
        ):
        """
        Runs the complete stock prediction pipeline for multi-step forecasting.
        """
        print(f"--- Starting Multi-Step ({n_outputs}) Pipeline for Ticker: {ticker} ---")

        # 1. Get Data
        data = get_stock_data(ticker, period, interval)
        if data.empty: print("Pipeline stopped: No data fetched."); return

        original_index = data.index

        # 2. Calculate Technical Indicators (Using the corrected version from previous step)
        data = calculate_technical_indicators(data) # Make sure this uses fillna=True
        if data.empty: print("Pipeline stopped: Data empty after indicators."); return

        available_cols = data.columns.tolist()
        actual_features_to_lag = [f for f in features_to_lag if f in available_cols]
        if not actual_features_to_lag:
             print("Warning: No specified features_to_lag exist. Skipping lag creation.")
        else:
            # 3. Create Lagged Features
            data = create_lagged_features(data, actual_features_to_lag, lag_periods)
            if data.empty: print("Pipeline stopped: Data empty after lagging."); return

        final_index = data.index
        final_columns = data.columns

        # 4. Preprocess Data (Scaling, Splitting, Sequencing for Multi-Step)
        try:
            X_train, y_train, X_test, y_test, scaler, feature_names, target_col_index = preprocess_data(
                data, target_column, sequence_length, n_outputs, test_split_ratio # Pass n_outputs
            )
        except ValueError as e:
            print(f"Pipeline stopped during preprocessing: {e}"); return
        except Exception as e:
             print(f"An unexpected error occurred during preprocessing: {e}"); return

        if X_train is None or X_test.shape[0] == 0: # Check if preprocessing failed or test set empty
            print("Pipeline stopped: Preprocessing returned None or generated empty test set."); return

        input_size = X_train.shape[2]
        output_size = n_outputs # <--- Model output size is n_outputs

        # 5. Create DataLoaders
        train_loader, test_loader = create_dataloaders(X_train, y_train, X_test, y_test, batch_size)

        # 6. Define Model, Loss, Optimizer
        model = LSTMModel(input_size, hidden_size, num_layers, output_size, dropout_prob) # Pass output_size
        criterion = nn.MSELoss()
        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

        print("\n--- Model Architecture ---")
        print(model)
        total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"Total Trainable Parameters: {total_params}")
        print("------------------------\n")


        # 7. Train Model
        # Use test_loader for validation during training in this setup
        trained_model, train_loss, val_loss = train_model(
            model, train_loader, test_loader, criterion, optimizer, num_epochs, patience
        )

        # 8. Evaluate Model on Test Set (Overall Loss)
        test_loss = evaluate_model(trained_model, test_loader, criterion)
        print(f"Final Test MSE Loss (Average over {n_outputs} steps): {test_loss:.6f}")
        print(f"Final Test RMSE Loss (Average over {n_outputs} steps): {np.sqrt(test_loss):.6f}")

        # 9. Make Predictions (Multi-Step)
        actuals_inv, preds_inv = make_predictions(
            trained_model, test_loader, scaler, input_size, target_col_index, n_outputs # Pass n_outputs
        )

        # 10. Calculate Error per Step
        step_errors = calculate_step_errors(actuals_inv, preds_inv, n_outputs)

        # 11. Plot Results
        plot_loss(train_loss, val_loss)
        plot_first_step_results(actuals_inv, preds_inv, title=f"{ticker} Stock Price Prediction (Test Set - Step 1 Only)")
        plot_error_by_step(step_errors)
        plot_multi_step_forecast_examples(actuals_inv, preds_inv, n_outputs, num_examples=5, title=f"{ticker} Multi-Step Forecast Examples")


        print(f"--- Pipeline for Ticker: {ticker} Finished ---")

        # Optional returns
        # return trained_model, scaler, train_loss, val_loss, actuals_inv, preds_inv, step_errors
    return (stock_prediction_pipeline,)


@app.cell
def _(stock_prediction_pipeline):
    stock_prediction_pipeline(
        ticker='ETH-USD',
        period='7d',
        interval='1m',
        features_to_lag=['close', 'volume', 'trend_macd_diff', 'momentum_rsi'],
        lag_periods=[1, 2, 3, 5, 7, 10], # Maybe more lags
        target_column='close',
        sequence_length=90,
        n_outputs=5,             # <--- Set prediction horizon
        test_split_ratio=0.15,
        hidden_size=128,
        num_layers=2,
        dropout_prob=0.25,
        # learning_rate=0.0005,
        learning_rate=0.001,
        num_epochs=120,           # Might need more/fewer epochs
        batch_size=32,
        patience=15
    )
    return


if __name__ == "__main__":
    app.run()
